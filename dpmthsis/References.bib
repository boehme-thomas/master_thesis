/home/david/Literatur/auger2009/auger2009.bib
@inproceedings{auger2009,
  address = {New York, NY, USA},
  author = {Auger, Anne},
  booktitle = {Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers},
  doi = {10.1145/1570256.1570342},
  isbn = {9781605585055},
  keywords = {adaptive search, benchmarking, evolutionary computation, black-box optimization, one-fifth success rule, evolution strategies},
  location = {Montreal, Qu\'{e}bec, Canada},
  numpages = {6},
  pages = {2447-–2452},
  publisher = {Association for Computing Machinery},
  series = {GECCO ’09},
  title = {Benchmarking the (1+1) Evolution Strategy with One-Fifth Success Rule on the {BBOB-2009} Function Testbed},
  url = {https://doi.org/10.1145/1570256.1570342},
  year = {2009},
}



/home/david/Literatur/butz2003b/butz2003b.bib
@article{butz2003b,
  author = {Martin V. Butz  and  David E. Goldberg  and  Kurian K. Tharakunnel},
  title = {Analysis and Improvement of Fitness Exploitation in {XCS}: Bounding Models, Tournament Selection, and Bilateral Accuracy},
  journal = {Evolutionary Computation},
  volume = {11},
  number = {3},
  pages = {239--277},
  year = {2003},
}
  doi = {10.1162/106365603322365298},
  URL = {https://doi.org/10.1162/106365603322365298},
  eprint = {https://doi.org/10.1162/106365603322365298},
  abstract = { The evolutionary learning mechanism in XCS strongly depends on its accuracy-based fitness approach. The approach is meant to result in an evolutionary drive from classifiers of low accuracy to those of high accuracy. Since, given inaccuracy, lower specificity often corresponds to lower accuracy, fitness pressure most often also results in a pressure towards higher specificity. Moreover, fitness pressure should cause the evolutionary process to be innovative in that it combines low-order building blocks of lower accurate classifiers, to higher-order building blocks with higher accuracy. This paper investigates how, when, and where accuracy-based fitness results in successful rule evolution in XCS. Along the way, a weakness in the current proportionate selection method in XCS is identified. Several problem bounds are derived that need to be obeyed to enable proper evolutionary pressure. Moreover, a fitness dilemma is identified that causes accuracy-based fitness to be misleading. Improvements are introduced to XCS to make fitness pressure more robust and overcome the fitness dilemma. Specifically, (1) tournament selection results in a much better fitness-bias exploitation, and (2) bilateral accuracy prevents the fitness dilemma. While the improvements stand for themselves, we believe they also contribute to the ultimate goal of an evolutionary learning system that is able to solve decomposable machine-learning problems quickly, accurately, and reliably. The paper also contributes to the further understanding of XCS in general and the fitness approach in XCS in particular. },


/home/david/Literatur/unity2019b.bib
@online{unity2019b,
  author  = {Danny Lange},
  lastaccessed = {2019-08-12},
  month   = {January},
  title   = {Obstacle Tower Challenge: Test the limits of intelligence systems},
  url     = {https://blogs.unity3d.com/2019/01/28/obstacle-tower-challenge-test-the-limits-of-intelligence-systems/},
  urldate = {2019-08-12},
  year    = {2019},
}


/home/david/Literatur/stein2019/stein2019.bib
@phdthesis{stein2019,
  author      = {Anthony Stein},
  title       = {Interpolation-Assisted Evolutionary Rule-Based Machine Learning - Strategies to Counter Knowledge Gaps in XCS-Based Self-Learning Adaptive Systems},
  type        = {doctoralthesis},
  school      = {Universit{\"a}t Augsburg},
  year        = {2019},
}


/home/david/Literatur/butz2004d/butz2004d.bib
@inproceedings{butz2004d,
  author = {Butz, Martin V. and Goldberg, David E. and Lanzi, Pier Luca},
  editor = {Deb, Kalyanmoy},
  title = {Bounding Learning Time in XCS},
  booktitle = {Genetic and Evolutionary Computation -- GECCO 2004},
  year = {2004},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {739--750},
  isbn = {978-3-540-24855-2},
}
  abstract = {It has been shown empirically that the XCS classifier system solves typical classification problems in a machine learning competitive way. However, until now, no learning time estimate has been derived analytically for the system. This paper introduces a time estimate that bounds the learning time of XCS until maximally accurate classifiers are found. We assume a domino convergence model in which each attribute is successively specialized to the correct value. It is shown that learning time in XCS scales polynomially in problem length and problem complexity and thus in a machine learning competitive way.},
  booktitle was checked


/home/david/Literatur/bernado-mansilla2005/bernado-mansilla2005.bib
@article{bernado-mansilla2005,
 author = {Bernad{\'o}-Mansilla, Ester and Ho, Tin Kam},
 title = {Domain of Competence of XCS Classifier System in Complexity Measurement Space},
 journal = {{IEEE} Transactions on Evolutionary Computation},
 volume = {9},
 number = {1},
 month = {Feb},
 year = {2005},
 pages = {82--104},
 numpages = {23},
 publisher = {{IEEE} Press},
 address = {Piscataway, NJ, USA},
}
 url = {http://dx.doi.org/10.1109/TEVC.2004.840153},
 issn = {1089-778X},
 issue_date = {February 2005},
 doi = {10.1109/TEVC.2004.840153},
 acmid = {2221714},




/home/david/Literatur/stalph2009b/stalph2009b.bib
@inproceedings{stalph2009b,
  address = {New York, NY, USA},
  author = {Stalph, Patrick O. and Butz, Martin V. and Goldberg, David E. and Llor\`{a}, Xavier},
  booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
  pages = {1315--1322},
  publisher = {ACM},
  series = {GECCO '09},
  title = {On the Scalability of XCS(F)},
  year = {2009},
}
  location = {Montreal, Qu\&\#233;bec, Canada},
  keywords = {function approximation, learning classifier systems, lwpr, recursive least squares, xcs},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/1569901.1570077},
  isbn = {978-1-60558-325-9},
  doi = {10.1145/1569901.1570077},
  acmid = {1570077},
  booktitle checked


/home/david/Literatur/erwig2006/erwig2006.bib
@article{erwig2006,
  author    = {Martin Erwig and Steve Kollmansberger},
  title     = {Functional Pearls: Probabilistic functional programming in Haskell},
  journal   = {J. Funct. Program.},
  volume    = {16},
  number    = {1},
  pages     = {21--34},
  year      = {2006},
  url       = {https://doi.org/10.1017/S0956796805005721},
  doi       = {10.1017/S0956796805005721},
  timestamp = {Sat, 27 May 2017 14:24:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/jfp/ErwigK06},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


/home/david/Literatur/hayashida2019/hayashida2019.bib
@article{hayashida2019,
    author = {Hayashida, Tomohiro and Nishizaki, Ichiro and Sekizaki, Shinya and Ogasawara, Yuki},
    title = {Development of a classifier system for a continuous environment},
    journal = {Electronics and Communications in Japan},
    volume = {102},
    number = {10},
    pages = {17--25},
    keywords = {classifier system, continuous value environment, machine learning, neural networks},
    doi = {10.1002/ecj.12209},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ecj.12209},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ecj.12209},
    year = {2019},
}
abstract = {Abstract A learning classifier system is an adaptive system that obtains a set of appropriate action rules that adapts to multistep problems by training action rules defined in if-then form by trial and error process, in a similar framework as reinforcement learning. Because of that the input signals of the classifier system are encoded into binary values, bit strings are often lengthened when dealing with such a problem that the state of the environment continuously changes. A neural network can treat with real values as input signal; however, it cannot be applied to multistep problems. This paper proposes a system that responds to problems such that the state of the environment continuously changes by combining a neural network and a classifier system, and actions are selected from multiple options, so that output can be defined as discrete values. In order to verify the effectiveness of the proposed system, this paper conducts several numerical experiments using benchmarks corresponding to multistep problems defined by continuous values.},


/home/david/Literatur/bull2002/bull2002.bib
@article{bull2002,
  author = {Bull, Larry},
  title = {On accuracy-based fitness},
  journal = {Soft Computing},
  year = {2002},
  volume = {6},
  number = {3},
  pages = {154--161},
}
  issn = {1432-7643},
  doi = {10.1007/s005000100112},
  url = {https://doi.org/10.1007/s005000100112},
  day = {01},
  month = {June},


/home/david/Literatur/bull2003b/bull2003b.bib
 @inproceedings{bull2003b,
 author={Larry Bull and Jacob Hurst},
 booktitle={The 2003 Congress on Evolutionary Computation, 2003. {CEC} '03.},
 title={A neural learning classifier system with self-adaptive constructivism},
 year={2003},
 volume={2},
 number={},
 pages={991--997 Vol. 2},
}


/home/david/Literatur/kovacs2000/kovacs2000.bib
@inproceedings{kovacs2000,
  address = {Berlin, Heidelberg},
  author = {Kovacs, Tim},
  booktitle = {Learning Classifier Systems},
  editor = {Lanzi, Pier Luca and Stolzmann, Wolfgang and Wilson, Stewart W.},
  isbn = {978-3-540-45027-6},
  pages = {143--160},
  publisher = {Springer Berlin Heidelberg},
  title = {Strength or Accuracy? Fitness Calculation in Learning Classifier Systems},
  year = {2000},
}
  abstract = {Wilson's XCS is a clear departure from earlier classifier systems in terms of the way it calculates the fitness of classifiers for use in the genetic algorithm. Despite the growing body of work on XCS and the advantages claimed for it there has been no detailed comparison of XCS and traditional strength-based systems. This work takes a step towards rectifying this situation by surveying a number of issues related to the change in fitness. I distinguish different definitions of overgenerality for strength and accuracy-based fitness and analyse some implications of the use of accuracy, including an apparent advantage in addressing the explore/exploit problem. I analyse the formation of strong overgenerals, a major problem for strength-based systems, and illustrate their dependence on biased reward functions. I consider motivations for biasing reward functions in single step environments, and show that non-trivial multi step environments have biased Q-functions. I conclude that XCS's accuracy-based fitness appears to have a number of significant advantages over traditional strength-based fitness.},


/home/david/Literatur/butz2007/butz2007.bib
@article{butz2007,
  author = {Butz, Martin V. and Goldberg, David E. and Lanzi, Pier Luca and Sastry, Kumara},
  title = {Problem Solution Sustenance in XCS: Markov Chain Analysis of Niche Support Distributions and the Impact on Computational Complexity},
  journal = {Genetic Programming and Evolvable Machines},
  year = {2007},
  volume = {8},
  number = {1},
  pages = {5--37},
  publisher = {Springer},
}
  month = {Mar},
  day = {01},
  issn = {1573-7632},
  doi = {10.1007/s10710-006-9012-8},
  url = {https://doi.org/10.1007/s10710-006-9012-8},
  abstract = {Michigan-style learning classifier systems iteratively evolve a distributed solution to a problem in the form of potentially overlapping subsolutions. Each problem niche is covered by subsolutions that are represented by a set of predictive rules, termed classifiers. The genetic algorithm is designed to evolve classifier structures that together cover the whole problem space and represent a complete problem solution. An obvious challenge for such an online evolving, distributed knowledge representation is to continuously sustain all problem subsolutions covering all problem niches, that is, to ensure niche support. Effective niche support depends both on the probability of reproduction and on the probability of deletion of classifiers in a niche. In XCS, reproduction is occurrence-based whereas deletion is support-based. In combination, niche support is assured effectively. In this paper we present a Markov chain analysis of the niche support in XCS, which we validate experimentally. Evaluations in diverse Boolean function settings, which require non-overlapping and overlapping solution structures, support the theoretical derivations. We also consider the effects of mutation and crossover on niche support. With respect to computational complexity, the paper shows that XCS is able to maintain (partially overlapping) niches with a computational effort that is linear in the inverse of the niche occurrence frequency.},


/home/david/Literatur/graves2014/2014 Graves, Wayne, Danihelka - Neural Turing Machines.bib
@article{DBLP:journals/corr/GravesWD14,
  author    = {Alex Graves and Greg Wayne and Ivo Danihelka},
  title     = {Neural Turing Machines},
  journal   = {CoRR},
  volume    = {abs/1410.5401},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.5401},
  timestamp = {Wed, 07 Jun 2017 14:42:19 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  bibsource = {dblp computer science bibliography, http://dblp.org\}
}


/home/david/Literatur/zhang2019/zhang2019.bib
@inproceedings{zhang2019,
    author={Zheming Zhang and Will N. Browne and Dale A. Carnegie},
    booktitle={2019 {IEEE} Congress on Evolutionary Computation (CEC)},
    title={{XCS} with Combined Reward Method ({XCSCR}) for Policy Search in Multistep Problems},
    year={2019},
    volume={},
    number={},
    pages={2982--2989},
    doi={10.1109/CEC.2019.8790115},
    month={June},
}
    keywords={learning (artificial intelligence);pattern classification;search problems;XCSCR;policy search;multistep problems;action policies;long-term rewards;policy learning process;short-term rewards;initial learning phase;global optimal policies;short-term reward mechanism;indiscriminate rewards;learning-rate switching mechanism;long-term positive rewards;policy searching process;learning step-threshold mechanism;optimum path-finding policies;reinforcement learning agent;XCS with combined reward method;Robots;Standards;Task analysis;Estimation;Mathematical model;Collision avoidance;Search problems;reward mechanism;scarce/sparse reward;Reinforcement Learning;multistep;maze problems;XCS;XCSCR},
    ISSN={null},


/home/david/Literatur/hackage.bib
@online{hackage,
  label = {Hac},
  title = {Hackage Homepage},
  url = {https://hackage.haskell.org/},
  urldate = {2017-07-14},
}
  author = {{Hackage}},
  year = {2015},


/home/david/Literatur/ji2019/ji2019.bib
@inproceedings{ji2019,
    author = {Ji, Hong-Ming and Chen, Liang-Yu and Hsiao, Tzu-Chien},
    title = {Real-time Detection of Internet Addiction Using Reinforcement Learning System},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319619.3326882},
    doi = {10.1145/3319619.3326882},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {1280–-1288},
    numpages = {9},
    keywords = {internet addiction, reinforcement learning system, extended classifier system with continuous real-coded variables, instantaneous respiratory frequency},
    location = {Prague, Czech Republic},
    series = {GECCO ’19},
}


/home/david/Literatur/hoare1983/hoare1983.bib
@article{hoare1983,
  author = {Hoare, C. A. R.},
  title = {An Axiomatic Basis for Computer Programming},
  journal = {Communications of the {ACM}},
  volume = {26},
  number = {1},
  year = {1983},
  pages = {53--56},
  publisher = {{ACM}},
  address = {New York, NY, USA},
}
 numpages = {4},
 issue_date = {Jan. 1983},
 month = jan,
 issn = {0001-0782},
 url = {http://doi.acm.org/10.1145/357980.358001},
 doi = {10.1145/357980.358001},
 acmid = {358001},
 keywords = {axiomatic method, formal language definition, machine-independent programming, program documentation, programming language design, proofs of program, proofs of programs, theory of programming},


/home/david/Literatur/naqvi2016/naqvi2016.bib
@inproceedings{naqvi2016,
  author = {Syed S. Naqvi and Will N. Browne},
  booktitle = {2016 {IEEE} Congress on Evolutionary Computation ({CEC})},
  title = {Adapting learning classifier systems to symbolic regression},
  year = {2016},
  pages = {2209--2216},
  doi = {10.1109/CEC.2016.7744061},
  month = {July},
}
  volume = {},
  number = {},
  ISSN = {null},
  keywords = {genetic algorithms;learning systems;pattern classification;regression analysis;symbol manipulation;learning classifier systems;symbolic regression;genetic programming;real-valued interval based conditions;code fragmented action;average absolute error;Evolutionary computation;Training;Testing},


/home/david/Literatur/chen2019/chen2019.bib
@inproceedings{chen2019,
    author = {Chen, Liang-Yu and Lee, Jia-Hua and Yang, Ya-Liang and Yeh, Ming-Tsung and Hsiao, Tzu-Chien},
    title = {Predicting the Remaining Useful Life of Plasma Equipment through {XCSR}},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319619.3326879},
    doi = {10.1145/3319619.3326879},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {1263--1270},
    numpages = {8},
    location = {Prague, Czech Republic},
    series = {GECCO ’19}
}
    keywords = {extended classifier system (xcs), fisher discriminant analysis (fda), digital radio frequency matching box (rf-mb), remaining useful life (rul)},


/home/david/Literatur/tatsumi2019/tatsumi2019.bib
@inproceedings{tatsumi2019,
    author = {Tatsumi, Takato and Takadama, Keiki},
    title = {{XCS-CR} for Handling Input, Output, and Reward Noise},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319619.3326863},
    doi = {10.1145/3319619.3326863},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {1303--1311},
    numpages = {9},
    keywords = {XCS, reward noise, output noise, input noise, accuracy criteria},
    location = {Prague, Czech Republic},
    series = {GECCO ’19},
}


/home/david/Literatur/chung2014/2014 Chung, Gulcehre, Cho, Bengio - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.bib
@article{DBLP:journals/corr/ChungGCB14,
  author    = {Junyoung Chung and {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and KyungHyun Cho and Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  timestamp = {Wed, 07 Jun 2017 14:40:04 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChungGCB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


/home/david/Literatur/rajagopalan2011/rajagopalan2011.bib
@INPROCEEDINGS{6102315, 
author={S. R. Rajagopalan and L. Sankar and S. Mohajer and H. V. Poor}, 
booktitle={2011 IEEE International Conference on Smart Grid Communications (SmartGridComm)}, 
title={Smart Meter Privacy: A Utility-Privacy Framework}, 
year={2011}, 
pages={190-195}, 
keywords={Gaussian processes;Markov processes;information theory;power meters;smart power grids;electricity load;end-user privacy;information theory;privacy-utility tradeoff problem;smart grid;smart meter measurements;smart meter privacy;stationary Gaussian Markov model;utility requirements;utility-privacy framework;Correlation;Data privacy;Distortion measurement;Home appliances;Load modeling;Privacy;Rate-distortion}, 
doi={10.1109/SmartGridComm.2011.6102315}, 
month={Oct},}


/home/david/Literatur/donyanavard2019/donyanavard2019.bib
@inproceedings{donyanavard2019,
    author = {Donyanavard, Bryan and M\"{u}ck, Tiago and Rahmani, Amir M. and Dutt, Nikil and Sadighi, Armin and Maurer, Florian and Herkersdorf, Andreas},
    title = {{SOSA}: Self-Optimizing Learning with Self-Adaptive Control for Hierarchical System-on-Chip Management},
    year = {2019},
    isbn = {9781450369381},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3352460.3358312},
    doi = {10.1145/3352460.3358312},
    booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
    pages = {685--698},
    numpages = {14},
    location = {Columbus, OH, USA},
    series = {MICRO ’52}
}


/home/david/Literatur/functionaldependencies.bib
@online{functionaldependencies,
  key = {{GHC} Users Guide},
  title = {Functional Dependencies},
  url = {https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/glasgow_exts.html#functional-dependencies},
  urldate = {2018-03-16},
  lastaccessed = {2018-03-16},
}
  year = {2014},
  author = {{GHC team}},
  titleaddon = {in GHC Users Guide},


/home/david/Literatur/stalph2010/stalph2010.bib
@inproceedings{stalph2010,
  address = {Berlin, Heidelberg},
  author = {Stalph, Patrick O. and Butz, Martin V.},
  booktitle = {Learning Classifier Systems},
  editor = {Bacardit, Jaume and Browne, Will N. and Drugowitsch, Jan and Bernad{\'o}-Mansilla, Ester and Butz, Martin V.},
  isbn = {978-3-642-17508-4},
  pages = {57--69},
  publisher = {Springer Berlin Heidelberg},
  title = {Current XCSF Capabilities and Challenges},
  year = {2010},
}
  abstract = {Function approximation is an important technique used in many different domains, including numerical mathematics, engineering, and neuroscience. The XCSF classifier system is able to approximate complex multi-dimensional function surfaces using a patchwork of simpler functions. Typically, locally linear functions are used due to the tradeoff between expressiveness and interpretability. This work discusses XCSF's current capabilities, but also points out current challenges that can hinder learning success. A theoretical discussion on when XCSF works is intended to improve the comprehensibility of the system. Current advances with respect to scalability theory show that the system constitutes a very effective machine learning technique. Furthermore, the paper points-out how to tune relevant XCSF parameters in actual applications and how to choose appropriate condition and prediction structures. Finally, a brief comparison to the Locally Weighted Projection Regression (LWPR) algorithm highlights positive as well as negative aspects of both methods.},



/home/david/Literatur/nguyen2019/nguyen2019.bib
@inproceedings{nguyen2019,
    author = {Nguyen, Trung B. and Browne, Will N. and Zhang, Mengjie},
    title = {Improvement of Code Fragment Fitness to Guide Feature Construction in {XCS}},
    year = {2019},
    isbn = {9781450361118},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3321707.3321751},
    doi = {10.1145/3321707.3321751},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
    pages = {428--436},
    numpages = {9},
    keywords = {LCS, XCS, XCSCFC, code fragment, genetic programming},
    location = {Prague, Czech Republic},
    series = {GECCO ’19},
}


/home/david/Literatur/karlsen2018/karlsen2018.bib
@article{karlsen2018,
  author    = {Matthew R. Karlsen and Sotiris Moschoyiannis},
  title     = {Evolution of control with learning classifier systems},
  journal   = {Applied Network Science},
  volume    = {3},
  number    = {1},
  pages     = {30:1--30:36},
  year      = {2018},
  url       = {https://doi.org/10.1007/s41109-018-0088-x},
  doi       = {10.1007/s41109-018-0088-x},
  timestamp = {Sat, 24 Nov 2018 11:58:14 +0100},
  biburl    = {https://dblp.org/rec/journals/ans/KarlsenM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


/home/david/Literatur/knuth1984/knuth1984.bib
@article{knuth1984,
  author  = {Knuth, Donald E.},
  journal = {The Computer Journal},
  number  = {2},
  pages   = {97--111},
  title   = {Literate Programming},
  volume  = {27},
  year    = {1984},
}
  eprint  = {/oup/backfile/content_public/journal/comjnl/27/2/10.1093/comjnl/27.2.97/2/270097.pdf}
  url     = {http://dx.doi.org/10.1093/comjnl/27.2.97},


/home/david/Literatur/sommer2016/sommer2016.bib
@incollection{sommer2016,
  author    = {Matthias Sommer and Sven Tomforde and J{\"o}rg H{\"a}hner},
  title     = {An Organic Computing Approach to Resilient Traffic Management},
  booktitle = {Autonomic Road Transport Support Systems},
  editor    = {Thomas Leo McCluskey and Apostolos Kotsialos and J{\"o}rg P. M{\"u}ller and Franziska Kl{\"u}gl and Omer F. Rana and Ren{\´e} Schumann},
  isbn      = {9783319258065},
  doi       = {10.1007/978-3-319-25808-9\_7},
  year      = {2016},
}


/home/david/Literatur/edakunni2009/edakunni2009.bib
@inproceedings{edakunni2009,
  address = {New York, NY, USA},
  author = {Edakunni, Narayanan Unny and Kovacs, Tim and Brown, Gavin and Marshall, James A. R.},
  title = {Modeling UCS As a Mixture of Experts},
  booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
  year = {2009},
  pages = {1187--1194},
  series = {GECCO '09},
  publisher = {{ACM}},
}
  booksubtitle = {Montreal, Qu{\'{e}}bec, Canada, July 8--12, 2009},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/1569901.1570061},
  doi = {10.1145/1569901.1570061},
  acmid = {1570061},
  keywords = {UCS, learning classifier system, mixture of experts, probabilistic modeling},
  isbn = {978-1-60558-325-9},
  crossref  = {DBLP:conf/gecco/2009g},
  biburl    = {https://dblp.org/rec/bib/conf/gecco/EdakunniKBM09},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  proceedings{DBLP:conf/gecco/2009g,
  editor    = {Franz Rothlauf},
  title     = {Genetic and Evolutionary Computation Conference, {GECCO} 2009, Proceedings,
              Montreal, Qu{\'{e}}bec, Canada, July 8-12, 2009},
  publisher = {{ACM}},
  year      = {2009},
  isbn      = {978-1-60558-325-9},
  timestamp = {Mon, 27 Jul 2009 11:06:29 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/gecco/2009g},
  bibsource = {dblp computer science bibliography, https://dblp.org},


/home/david/Literatur/butz2006b/butz2006b.bib
@article{butz2006b,
  author = {Butz, Martin V. and Pelikan, Martin and Llorà, Xavier and Goldberg, David E.},
  journal = {Evolutionary Computation},
  number = {3},
  pages = {345--380},
  title = {Automated Global Structure Extraction for Effective Local Building Block Processing in XCS},
  volume = {14},
  year = {2006},
}
  doi = {10.1162/evco.2006.14.3.345},
  url = {https://doi.org/10.1162/evco.2006.14.3.345},
  eprint = {https://doi.org/10.1162/evco.2006.14.3.345},
  abstract = { Abstract Learning Classifier Systems (LCSs), such as the accuracy-based XCS, evolve distributed problem solutions represented by a population of rules. During evolution, features are specialized, propagated, and recombined to provide increasingly accurate subsolutions. Recently, it was shown that, as in conventional genetic algorithms (GAs), some problems require efficient processing of subsets of features to find problem solutions efficiently. In such problems, standard variation operators of genetic and evolutionary algorithms used in LCSs suffer from potential disruption of groups of interacting features, resulting in poor performance. This paper introduces efficient crossover operators to XCS by incorporating techniques derived from competent GAs: the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of simple crossover operators such as uniform crossover or one-point crossover, ECGA or BOA-derived mechanisms are used to build a probabilistic model of the global population and to generate offspring classifiers locally using the model. Several offspring generation variations are introduced and evaluated. The results show that it is possible to achieve performance similar to runs with an informed crossover operator that is specifically designed to yield ideal problem-dependent exploration, exploiting provided problem structure information. Thus, we create the first competent LCSs, XCS/ECGA and XCS/BOA, that detect dependency structures online and propagate corresponding lower-level dependency structures effectively without any information about these structures given in advance. },


/home/david/Literatur/orriols-puig2007/orriols-puig2007.bib
@inproceedings{orriols-puig2007,
 address = {New York, NY, USA},
 author = {Orriols-Puig, Albert and Sastry, Kumara and Lanzi, Pier Luca and Goldberg, David E. and Bernad\'{o}-Mansilla, Ester},
 booktitle = {Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation},
 location = {London, England},
 pages = {1846--1853},
 publisher = {ACM},
 series = {GECCO '07},
 title = {Modeling Selection Pressure in XCS for Proportionate and Tournament Selection},
 year = {2007},
}
 numpages = {8},
 acmid = {1277325},
 doi = {10.1145/1276958.1277325},
 isbn = {978-1-59593-697-4},
 keywords = {LCS, XCS, proportionate selection, tournament selection},
 url = {http://doi.acm.org/10.1145/1276958.1277325},


/home/david/Literatur/lanzi2008/lanzi2008.bib
@inproceedings{lanzi2008,
  address = {Berlin, Heidelberg},
  author = {Lanzi, Pier Luca and Loiacono, Daniele},
  booktitle = {Learning Classifier Systems},
  editor = {Bacardit, Jaume and Browne, Will N. and Drugowitsch, Jan and Bernad{\'o}-Mansilla, Ester and Butz, Martin V.},
  isbn = {978-3-642-17508-4},
  pages = {1--20},
  publisher = {Springer Berlin Heidelberg},
  title = {Speeding Up Matching in Learning Classifier Systems Using {CUDA}},
  year = {2010},
}
  abstract = {We investigate the use of NVIDIA's Compute Unified Device Architecture (CUDA) to speed up matching in classifier systems. We compare CUDA-based matching and CPU-based matching on (i) real inputs using interval-based conditions and on (ii) binary inputs using ternary conditions. Our results show that on small problems, due to the memory transfer overhead introduced by CUDA, matching is faster when performed using the CPU. As the problem size increases, CUDA-based matching can outperform CPU-based matching resulting in a 3-12{\texttimes} speedup when the interval-based representation is applied to match real-valued inputs and a 20-50{\texttimes} speedup for ternary-based representation.},



/home/david/Literatur/takadama2019/takadama2019.bib
@inproceedings{takadama2019,
    author={Keiki Takadama and Daichi Yamazaki and Masaya Nakata and Hiroyuki Sato},
    booktitle={2019 {IEEE} Congress on Evolutionary Computation ({CEC})},
    title={Complex-Valued-based Learning Classifier System for {POMDP} Environments},
    year={2019},
    volume={},
    number={},
    pages={1852--1859},
    keywords={learning (artificial intelligence);optimisation;pattern classification;Q-values;CVRL;optimal policy;POMDP environments;complex-valued reinforcement learning;complex-valued-based learning classifier system;CVLCS;Zero current switching;Reinforcement learning;Markov processes;Memory management;Probabilistic logic;Time series analysis;Evolutionary computation;learning classifier system;complex-value;partially observable Markov decision processes;reinforcement learning;evolutionary computation},
    doi={10.1109/CEC.2019.8790083},
    month={June},
}
issn={null},


/home/david/Literatur/foerster2018b/foerster2018b.bib
@inproceedings{foerster2018b,
  author    = {Jakob N. Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson},
  editor    = {Sheila A. McIlraith and Kilian Q. Weinberger},
  title     = {Counterfactual Multi-Agent Policy Gradients},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018},
  pages     = {2974--2982},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17193},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/FoersterFANW18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


/home/david/Literatur/liu2019/liu2019.bib
@article{liu2019,
    author = {Boyi {Liu} and Lujia {Wang} and Ming {Liu}},
    doi = {10.1109/LRA.2019.2931179},
    issn = {2377-3774},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {Oct},
    number = {4},
    pages = {4555--4562},
    title = {Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems},
    volume = {4},
    year = {2019},
}
    keywords = {cloud computing;learning (artificial intelligence);mobile robots;path planning;Web sites;transfer learning methods;LFRL;robot navigation;cloud robotic system deployment;learning architecture;knowledge fusion algorithm;lifelong federated reinforcement learning;human cognitive science;cloud robotic navigation-learning Website;Navigation;Reinforcement learning;Task analysis;Cloud computing;Training;Robot kinematics;Deep learning in robotics and automation;autonomous vehicle navigation;AI-based methods},


/home/david/Literatur/drugowitsch2008b/drugowitsch2008b.bib
@book{drugowitsch2008b,
  author    = {Jan Drugowitsch},
  title     = {Design and Analysis of Learning Classifier Systems - {A} Probabilistic Approach},
  series    = {Studies in Computational Intelligence},
  volume    = {139},
  publisher = {Springer Berlin Heidelberg},
  year      = {2008},
  address   = {Berlin, Heidelberg},
}
  url       = {https://doi.org/10.1007/978-3-540-79866-8},
  doi       = {10.1007/978-3-540-79866-8},
  isbn      = {978-3-540-79865-1},
  timestamp = {Tue, 16 May 2017 14:24:30 +0200},
  biburl    = {https://dblp.org/rec/bib/series/sci/2008-139},
  bibsource = {dblp computer science bibliography, https://dblp.org},


/home/david/Literatur/jordan1994/jordan1994.bib
@article{jordan1994,
author = {Jordan, Michael I. and Jacobs, Robert A.},
title = {Hierarchical Mixtures of Experts and the {EM} Algorithm},
journal = {Neural Computation},
volume = {6},
number = {2},
pages = {181--214},
year = {1994},
doi = {10.1162/neco.1994.6.2.181},
URL = {https://doi.org/10.1162/neco.1994.6.2.181},
eprint = {https://doi.org/10.1162/neco.1994.6.2.181},
}
abstract = { We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. }


/home/david/Literatur/wilson2004/wilson2004.bib
@inproceedings{wilson2004,
  address = {Berlin, Heidelberg},
  author = {Wilson, Stewart W.},
  booktitle = {Genetic and Evolutionary Computation -- GECCO 2004},
  editor = {Deb, Kalyanmoy},
  isbn = {978-3-540-24855-2},
  pages = {824--835},
  publisher = {Springer Berlin Heidelberg},
  title = {Classifier systems for continuous payoff environments},
  year = {2004},
}
  abstract = {Recognizing that many payoff functions are continuous and depend on the input state x, the classifier system architecture XCS is extended so that a classifier's prediction is a linear function of x. On a continuous nonlinear problem, the extended system, XCS-LP, exhibits high performance and low error, as well as dramatically smaller evolved populations compared with XCS. Linear predictions are seen as a new direction in the quest for powerful generalization in classifier systems.},


/home/david/Literatur/yadollahi2019/yadollahi2019.bib
@inproceedings{yadollahi2019,
    author={Mohammad M. Yadollahi and Farzaneh Shoeleh and Elham Serkani and Afsaneh Madani and Hossein Gharaee},
    booktitle={2019 5th International Conference on Web Research ({ICWR})},
    title={An Adaptive Machine Learning Based Approach for Phishing Detection Using Hybrid Features},
    year={2019},
    volume={},
    number={},
    pages={281--286},
    doi={10.1109/ICWR.2019.8765265},
    month={April},
}
    ISSN={null},
    keywords={computer crime;Internet;learning (artificial intelligence);Web sites;hybrid features;World Wide Web;phishing attackers;anti-phishing system;reliable detection system;Web threats;adaptive machine learning based approach;intelligent phishing detection;phishing Websites;feature-rich machine learning technique;URLs;Webpages source code;Phishing;Feature extraction;Uniform resource locators;Hypertext systems;Machine learning;Blacklisting;Real-time systems;Cyber Security;Phishing Detection;Web Security;Learning Classifier System},


/home/david/Literatur/butz2004/butz2004.bib
@article{butz2004,
  author={Butz, Martin V. and Kovacs, Tim and Lanzi, Pier Luca and Wilson, Stewart W.},
  journal={{IEEE} Transactions on Evolutionary Computation},
  number={1},
  pages={28--46},
  publisher={{IEEE}},
  month = {2},
  title={Toward a Theory of Generalization and Learning in {XCS}},
  volume={8},
  year={2004},
}


/home/david/Literatur/kovacs1998a/kovacs1998a.bib
@InProceedings{kovacs1998a,
  address = {London},
  author = {Kovacs, Tim},
  booktitle = {Soft Computing in Engineering Design and Manufacturing},
  editor = {Chawdhry, P. K. and Roy, R. and Pant, R. K.},
  isbn = {978-1-4471-0427-8},
  pages = {59--68},
  publisher = {Springer London},
  title = {XCS Classifier System Reliably Evolves Accurate, Complete, and Minimal Representations for Boolean Functions},
  year = {1998},
}
abstract = {Wilson's recent XCS classifier system forms complete mappings of the payoff environment in the reinforcement learning tradition thanks to its accuracy based fitness. According to Wilson's Generalization Hypothesis, XCS has a tendency towards generalization. With the XCS Optimality Hypothesis, I suggest that XCS systems can evolve optimal populations (representations); populations which accurately map all input/action pairs to payoff predictions using the smallest possible set of non-overlapping classifiers. The ability of XCS to evolve optimal populations for boolean multiplexer problems is demonstrated using condensation, a technique in which evolutionary search is suspended by setting the crossover and mutation rates to zero. Condensation is automatically triggered by self-monitoring of performance statistics, and the entire learning process is terminated by autotermination. Combined, these techniques allow a classifier system to evolve optimal representations of boolean functions without any form of supervision. A more complex but more robust and efficient technique for obtaining optimal populations called subset extraction is also presented and compared to condensation.},



/home/david/Literatur/drugowitsch2005/drugowitsch2005.bib
@inproceedings{drugowitsch2005,
author = {Drugowitsch, Jan and Barry, Alwyn M.},
title = {{XCS} with Eligibility Traces},
year = {2005},
isbn = {1595930108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1068009.1068322},
doi = {10.1145/1068009.1068322},
booktitle = {Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation},
pages = {1851--1858},
numpages = {8},
keywords = {temporal-difference learning, XCS, q-learning, eligibility traces, LCS},
location = {Washington DC, USA},
series = {GECCO ’05},
}

  




/home/david/Literatur/dewitt2019/dewitt2019.bib
@incollection{dewitt2019,
    title = {Multi-Agent Common Knowledge Reinforcement Learning},
    author = {Schroeder de Witt, Christian and Foerster, Jakob and Farquhar, Gregory and Torr, Philip and Boehmer, Wendelin and Whiteson, Shimon},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {9927--9939},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/9184-multi-agent-common-knowledge-reinforcement-learning.pdf},
}


/home/david/Literatur/wilson1995/wilson1995.bib
@article{wilson1995,
  journal={Evolutionary Computation},
  author = {Wilson, Stewart W.},
  title = {Classifier Fitness Based on Accuracy},
  volume = {3},
  number = {2},
  year = {1995},
  pages = {149--175},
  publisher = {MIT Press},
}
  keywords = {Classifier systems, accuracy, fitness, generalization, mapping, niche genetic algorithm, restricted mating, strength},
  issn = {1063-6560},
  address = {Cambridge, MA, USA},
  issue_date = {Summer 1995},
  url = {http://dx.doi.org/10.1162/evco.1995.3.2.149},
  doi = {10.1162/evco.1995.3.2.149},
  numpages = {27},
  acmid = {1326693},
  month = {June},


/home/david/Literatur/multiparamtypeclasses.bib
@online{multiparamtypeclasses,
  author = {{GHC Team}},
  title = {Multi-parameter type classes},
  titleaddon = {in {Glasgow Haskell Compiler} Users Guide},
  url = {https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/glasgow_exts.html#multi-parameter-type-classes},
  urldate = {2017-07-13}
}
  year = {2014},


/home/david/Literatur/molnar2020/molnar2020.bib
@book{molnar2020,
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  title      = {Interpretable Machine Learning},
  year       = {2020},
}


/home/david/Literatur/bull2005/bull2005.bib
@Inbook{bull2005,
  address = {Berlin, Heidelberg},
  author = {Bull, Larry},
  bookTitle = {Foundations of Learning Classifier Systems},
  editor = {Bull, Larry and Kovacs, Tim},
  pages = {63--89},
  publisher = {Springer Berlin Heidelberg},
  title = {Two Simple Learning Classifier Systems},
  year = {2005},
}
  doi = {10.1007/11319122_4},
  isbn = {978-3-540-32396-9},
  url = {https://doi.org/10.1007/11319122_4},
  abstract = {Since its introduction Holland's Learning Classifier System (LCS) [Holland, 1976] has inspired much research into `genetics-based' machine learning [Goldberg, 1989].Given the complexity of the developed system [Holland, 1986], simplified versions have previously been presented (e.g., [Goldberg, 1989][Wilson, 1994]) to improve both performance and understanding. It has recently been shown that Wilson's simpler `zeroth-level' system (ZCS) [Wilson, 1994] can perform optimally [Bull {\&} Hurst, 2002] but ``it would appear that the interaction between the rate of rule updates and the fitness sharing process is critical'' [ibid.]. In this chapter, a simplified version of ZCS is explored - termed a `minimal' classifier system, MCS.},


/home/david/Literatur/bull2000/bull2000.bib
@inproceedings{bull2000,
  address = {Berlin, Heidelberg},
  author = {Bull, Larry},
  booktitle = {Advances in Learning Classifier Systems},
  editor = {Luca Lanzi, Pier and Stolzmann, Wolfgang and Wilson, Stewart W.},
  isbn = {978-3-540-44640-8},
  pages = {21--28},
  publisher = {Springer Berlin Heidelberg},
  title = {Simple Markov Models of the Genetic Algorithm in Classifier Systems: Accuracy-Based Fitness},
  year = {2001},
}
  abstract = {Michigan-style Classifier Systems use Genetic Algorithms to facilitate rule-discovery, where rule fitness has traditionally been predictionbased. Current research has shifted to the use of accuracy-based fitness. This paper presents a simple Markov model of the algorithm in such systems, allowing comparison between the two forms of rule utility measure. Using a single-step task the previously discussed benefits of accuracy over prediction are clearly shown with regard to overgeneral rules. The effects of a niche-based algorithm (maximal generality) are also briefly examined, as are the effects of mutation under the two fitness schemes.},



/home/david/Literatur/llora2006/llora2006.bib
@inproceedings{llora2006,
  acmid = {1144244},
  address = {New York, NY, USA},
  author = {Llor\`{a}, Xavier and Sastry, Kumara},
  booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
  doi = {10.1145/1143997.1144244},
  isbn = {1-59593-186-4},
  location = {Seattle, Washington, USA},
  numpages = {8},
  pages = {1513--1520},
  publisher = {ACM},
  series = {GECCO '06},
  title = {Fast Rule Matching for Learning Classifier Systems via Vector Instructions},
  url = {http://doi.acm.org/10.1145/1143997.1144244},
  year = {2006},
}
  keywords = {Altivec, SSE2, fast rule matching, learning classifier systems, vector operations},



/home/david/Literatur/kim2019/kim2019.bib
@article{kim2019,
    title = {Exploiting deep convolutional neural networks for a neural-based learning classifier system},
    journal = {Neurocomputing},
    volume = {354},
    pages = {61--70},
    year = {2019},
    note = {Recent Advancements in Hybrid Artificial Intelligence Systems},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2018.05.137},
    url = {http://www.sciencedirect.com/science/article/pii/S0925231219304606},
    author = {Ji-Yoon Kim and Sung-Bae Cho},
    keywords = {Convolutional neural networks, Inception module, Residual learning, Neural-based learning classifier systems, Database intrusion detection},
    abstract = {Classification is a key factor in accuracy, simplicity, and expressiveness, and it is difficult to optimize all of these factors at the same time. The learning classifier system (LCS) is a suitable technique for addressing an adaptive classification problem. It is a combination of fast approximation and evolutionary optimization techniques. A neural-based learning classifier system (N-LCS) includes an architecture for maintaining expressiveness by incorporating neural networks into a supervised classifier system, which is also an LCS specializing in classification studies. In recent years, studies using deep artificial neural networks have been actively conducted. In particular, deep convolutional neural networks (CNN) provide a powerful representation in an extremely fundamental method and demonstrates the high performance in various domains. In this paper, we exploit various deep CNN architectures in convolutional neural-based learning classifier systems (CN-LCS) combining the CNN and LCS to explore the possibility of a CN-LCS. By using various CNNs as an action of a classifier in an N-LCS, better classification accuracy can be obtained and classifier can be optimized. Experimental results show that our models achieve the higher performance than N-LCS for database intrusion detection as well as two other datasets, and extract effective features from deep representation by projecting data samples learned by several deep CNN models into the feature space.}
}


/home/david/Literatur/sanchez2015/sanchez2015.bib
@inproceedings{sanchez2015,
  ISSN={2325-4289},
  author={Pablo Garćıa-Sánchez and Alberto Tonda and Antonio M. Mora and Giovanni Squillero and J.J. Merelo},
  booktitle={2015 IEEE Conference on Computational Intelligence and Games (CIG)},
  doi={10.1109/CIG.2015.7317940},
  month={Aug},
  number={},
  pages={284-291},
  title={Towards automatic {StarCraft} strategy generation using genetic programming},
  volume={},
  year={2015},
}
keywords={C++ language;computer games;genetic algorithms;software agents;automatic StarCraft strategy generation;genetic programming;real-time strategy games;artificial intelligence;asymmetric balanced factions;technology trees;tactics;evolutionary computation;squad formation;map exploration;StarCraftGP;squad composition;bot behavior;C++ classes;OpprimoBot open-source framework;competitive strategy;Zerg bot;human-designed bots;Games;Artificial intelligence;Buildings;Optimization;Measurement;Sociology;Statistics},



/home/david/Literatur/drugowitsch2019/drugowitsch2019.bib
@article{drugowitsch2019,
  doi = {10.21105/joss.01359},
  url = {https://doi.org/10.21105/joss.01359},
  year = {2019},
  publisher = {The Open Journal},
  volume = {4},
  number = {38},
  pages = {1359},
  author = {Jan Drugowitsch},
  title = {{VBLinLogit}: Variational Bayesian linear and logistic regression},
  journal = {Journal of Open Source Software}
}



/home/david/Literatur/vinyals2017b.bib
@online{vinyals2017b,
  author  = {Oriol Vinyals and Stephen Gaffney and Timo Ewalds},
  title   = {{DeepMind} and {Blizzard} open StarCraft {II} as an {AI} research environment},
  month   = {August},
  year    = {2017},
  url     = {https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/},
  urldate = {2019-08-05},
  lastaccessed = {2019-08-05},
}


/home/david/Literatur/handl2006/handl2006.bib
@article{handl2006,
  author = {Handl, J. and Knowles, J. and Dorigo, M.},
  title = {Ant-Based Clustering and Topographic Mapping},
  journal = {Artificial Life},
  volume = {12},
  number = {1},
  pages = {35--62},
  year = {2006},
  doi = {10.1162/106454606775186400},
  URL = {https://doi.org/10.1162/106454606775186400},
  eprint = {https://doi.org/10.1162/106454606775186400},
}
abstract = { Ant-based clustering and sorting is a nature-inspired heuristic first introduced as a model for explaining two types of emergent behavior observed in real ant colonies. More recently, it has been applied in a data-mining context to perform both clustering and topographic mapping. Early work demonstrated some promising characteristics of the heuristic but did not extend to a rigorous investigation of its capabilities. We describe an improved version, called ATTA, incorporating adaptive, heterogeneous ants, a time-dependent transporting activity, and a method (for clustering applications) that transforms the spatial embedding produced by the algorithm into an explicit partitioning. ATTA is then subjected to the most rigorous experimental evaluation of an ant-based clustering and sorting algorithm undertaken to date: we compare its performance with standard techniques for clustering and topographic mapping using a set of analytical evaluation functions and a range of synthetic and real data collections. Our results demonstrate the ability of ant-based clustering and sorting to automatically identify the number of clusters inherent in a data collection, and to produce high quality solutions; indeed, we show that it is particularly robust for clusters of differing sizes and for overlapping clusters. The results obtained for topographic mapping are, however, disappointing. We provide evidence that the solutions generated by the ant algorithm are barely topology-preserving, and we explain in detail why results have—in spite of this—been misinterpreted (much more positively) in previous research. }


/home/david/Literatur/foerster2018/foerster2018.bib
@phdthesis{jakob2018,
  year = {2018},
  edition = {},
  number = {},
  journal = {},
  pages = {},
  publisher = {University of Oxford},
  school = {University of Oxford},
  title = {Deep multi-agent reinforcement learning},
  volume = {},
  author = {Foerster, Jakob N.},
  editor = {},
  series = {}
}


/home/david/Literatur/jain1999/jain1999.bib
@article{jain1999,
 author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
 title = {Data Clustering: A Review},
 journal = {ACM Comput. Surv.},
 issue_date = {Sept. 1999},
 volume = {31},
 number = {3},
 month = sep,
 year = {1999},
 issn = {0360-0300},
 pages = {264--323},
 numpages = {60},
 url = {http://doi.acm.org/10.1145/331499.331504},
 doi = {10.1145/331499.331504},
 acmid = {331504},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster analysis, clustering applications, exploratory data analysis, incremental clustering, similarity indices, unsupervised learning},
}


/home/david/Literatur/nordsieck2019/nordsieck2019.bib
@inproceedings{nordsieck2019,
    author = {Richard Nordsieck and Michael Heider and Andreas Angerer and Jörg Hähner},
    title = {Towards Automated Parameter Optimisation of Machinery by Persisting Expert Knowledge},
    booktitle = {Proceedings of the 16th International Conference on Informatics in Control, Automation and Robotics -- Volume 1: ICINCO},
    year = {2019},
    pages = {406--413},
    publisher = {SciTePress},
    organization = {INSTICC},
    doi = {https://doi.org/10.5220/0007953204060413},
    isbn = {978-989-758-380-3},
}


/home/david/Literatur/knuth1963/knuth1963.bib
@phdthesis{knuth1963,
  title = {Finite semifields and projective planes},
  year = {1963},
  author = {Donald E. Knuth}
}


/home/david/Literatur/wilson2001/wilson2001.bib
@inproceedings{wilson2001,
  author = {Wilson, Stewart W.},
  editor = {Luca Lanzi, Pier and Stolzmann, Wolfgang and Wilson, Stewart W.},
  title = {Mining Oblique Data with {XCS}},
  booktitle = {Advances in Learning Classifier Systems},
  year = {2001},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {158--174},
  isbn = {978-3-540-44640-8}
}
  abstract = {The classifier system XCS was investigated for data mining applications where the dataset discrimination surface (DS) is generally oblique to the attribute axes. Despite the classifiers' hyper-rectangular predicates, XCS reached 100{\%} performance on synthetic problems with diagonal DS's and, in a train/test experiment, competitive performance on the Wisconsin Breast Cancer dataset. Final classifiers in an extended WBC learning run were interpretable to suggest dependencies on one or a few attributes. For data mining of numeric datasets with partially oblique discrimination surfaces, XCS shows promise from both performance and pattern discovery viewpoints.},


/home/david/Literatur/loiacono2008/loiacono2008.bib
@inproceedings{loiacono2008,
  address = {Berlin, Heidelberg},
  author = {Loiacono, Daniele and Drugowitsch, Jan and Barry, Alwyn M. and Lanzi, Pier Luca},
  booktitle = {Learning Classifier Systems},
  editor = {Bacardit, Jaume and Bernad{\'o}-Mansilla, Ester and Butz, Martin V. and Kovacs, Tim and Llor{\`a}, Xavier and Takadama, Keiki},
  isbn = {978-3-540-88138-4},
  pages = {117--135},
  publisher = {Springer Berlin Heidelberg},
  title = {Analysis and Improvements of the Classifier Error Estimate in XCSF},
  year = {2008},
}
  abstract = {The estimation of the classifier error plays a key role in accuracy-based learning classifier systems. In this paper we study the current definition of the classifier error in XCSF and discuss the limitations of the algorithm that is currently used to compute the classifier error estimate from online experience. Subsequently, we introduce a new definition for the classifier error and apply the Bayes Linear Analysis framework to find a more accurate and reliable error estimate. This results in two incremental error estimate update algorithms that we compare empirically to the performance of the currently applied approach. Our results suggest that the new estimation algorithms can improve the generalization capabilities of XCSF, especially when the action-set subsumption operator is used.},



/home/david/Literatur/unity2019.bib
@online{unity2019,
  author  = {Unity GitHub Repository},
  lastaccessed = {2019-08-06},
  month   = {February},
  title   = {Obstacle Tower Challenge Official Rules},
  url     = {https://gitlab.aicrowd.com/unity/obstacle-tower-challenge-resources/blob/master/Rules.md},
  urldate = {2019-08-06},
  year    = {2019},
}


/home/david/Literatur/monadrandom.bib
@online{monadrandom,
  author = {Brent Yorgey and others},
  title = {The \emph{MonadRandom} {Haskell} package},
  year = {2018},
  url = {https://hackage.haskell.org/package/MonadRandom-0.5.1},
  urldate = {2018-03-22},
  lastaccessed = {2018-03-22},
}


/home/david/Literatur/preen2019/preen2019.bib
@misc{preen2019,
    title={Autoencoding with {XCSF}},
    author={Richard J. Preen and Stewart W. Wilson and Larry Bull},
    year={2019},
    eprint={1910.10579},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}


/home/david/Literatur/butz2005b/butz2005b.bib
@article{butz2005b,
  author = "Butz, Martin V. and Sastry, Kumara and Goldberg, David E.",
  day = "01",
  doi = "10.1007/s10710-005-7619-9",
  issn = "1573-7632",
  journal = "Genetic Programming and Evolvable Machines",
  month = "Mar",
  number = "1",
  pages = "53--77",
  title = "Strong, Stable, and Reliable Fitness Pressure in XCS due to Tournament Selection",
  url = "https://doi.org/10.1007/s10710-005-7619-9",
  volume = "6",
  year = "2005",
}


/home/david/Literatur/abbasi2020/abbasi2020.bib
@article{abbasi2020,
    title = {Efficient resource management and workload allocation in fog–cloud computing paradigm in {IoT} using learning classifier systems},
    journal = {Computer Communications},
    volume = {153},
    pages = {217--228},
    year = {2020},
    issn = {0140-3664},
    doi = {https://doi.org/10.1016/j.comcom.2020.02.017},
    url = {http://www.sciencedirect.com/science/article/pii/S0140366419318274},
    author = {Mahdi Abbasi and Mina Yaghoobikia and Milad Rafiee and Alireza Jolfaei and Mohammad R. Khosravi},
    keywords = {Fog computing, Internet of Things, Machine learning, Load distribution, Renewable power source, Cost},
}
    abstract = {With the rapid growth in network-connected computing devices, the Internet of Things (IoT) has progressed in terms of size and speed. Subsequently, the amount of produced data and computation loads has increased dramatically. A solution to handle this huge volume of workloads is cloud computing in which a considerable delay exists in the processing load and this has remained a concern in the field of distributed computing networks. Processing workloads at the edge of the network can reduce the response time while at the same time imposing energy constraints by bringing the task of load processing from data centers, which are supplied by electrical energy sources, to the network edges which are only supported by limited energies of batteries. Therefore, workloads need to be distributed evenly between the clouds and the edges of the network. In this paper, two methods based on XCS learning classifier systems (LCS), namely, XCS and BCM-XCS, are proposed to balance the power consumption at the edge of the network and to reduce delays in the processing of workloads. The results of our experiments are indicative of the superiority of BCM-XCS over the basic XCS-based method. The proposed methods distribute the workloads in a way that the delay in their processing and the communication delay between the cloud and fog nodes are both minimized. In addition to considerable advantages in controlling the fluctuations of the processing delay, the proposed methods can simultaneously reduce the processing delay by 42% by using a moderate power consumption at the edge of the network. The proposed methods can also recharge the renewable batteries used at the edge of the network about 18 percent more than the best state-of-the-art method.}


/home/david/Literatur/osullivan2008/osullivan2008.bib
@book{osullivan2008,
  author    = {Bryan O'Sullivan and John Goerzen and Don Stewart},
  title     = {Real World Haskell},
  publisher = {O'Reilly Media},
  year      = {2008},
}
  url       = {http://book.realworldhaskell.org/},
  address (2009 edition) = {Sebastopol, CA, USA},
  isbn      = {978-0-596-51498-3},
  timestamp = {Fri, 13 Jan 2012 14:26:09 +0100},
  biburl    = {https://dblp.org/rec/bib/books/daglib/0021990},
  bibsource = {dblp computer science bibliography, https://dblp.org}


/home/david/Literatur/bacardit2008/bacardit2008.bib
@inproceedings{bacardit2008,
  author = {Bacardit, Jaume and Bernad{\'o}-Mansilla, Ester and Butz, Martin V.},
  editor = {Bacardit, Jaume and Bernad{\'o}-Mansilla, Ester and Butz, Martin V. and Kovacs, Tim and Llor{\`a}, Xavier and Takadama, Keiki},
  title = {Learning Classifier Systems: Looking Back and Glimpsing Ahead},
  booktitle = {Learning Classifier Systems},
  year = {2008},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {1--21},
  isbn = {978-3-540-88138-4},
}
  abstract = {Over the recent years, research on Learning Classifier Systems (LCSs) got more and more pronounced and diverse. There have been significant advances of the LCS field on various fronts including system understanding, representations, computational models, and successful applications. In comparison to other machine learning techniques, the advantages of LCSs have become more pronounced: (1) rule-comprehensibility and thus knowledge extraction is straightforward; (2) online learning is possible; (3) local minima are avoided due to the evolutionary learning component; (4) distributed solution representations evolve; or (5) larger problem domains can be handled. After the tenth edition of the International Workshop on LCSs, more than ever before, we are looking towards an exciting future. More diverse and challenging applications, efficiency enhancements, studies of dynamical systems, and applications to cognitive control approaches appear imminent. The aim of this paper is to provide a look back at the LCS field, whereby we place our emphasis on the recent advances. Moreover, we take a glimpse ahead by discussing future challenges and opportunities for successful system applications in various domains.},



/home/david/Literatur/wilson2007/wilson2007.bib
@inproceedings{wilson2007,
  author = {Wilson, Stewart W.},
  editor = {Kovacs, Tim and Llor{\`a}, Xavier and Takadama, Keiki and Lanzi, Pier Luca and Stolzmann, Wolfgang and Wilson, Stewart W.},
  title = {Three Architectures for Continuous Action},
  booktitle = {Learning Classifier Systems},
  year = {2007},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {239--257},
  isbn = {978-3-540-71231-2}
}
  abstract = {Three classifier system architectures are introduced that permit the systems to have continuous (non-discrete) actions. One is based on interpolation, the second on an actor-critic paradigm, and the third on treating the action as a continuous variable homogeneous with the input. While the last architecture appears most interesting and promising, all three offer potential directions toward continuous action, a goal that classifier systems have hardly addressed.},



/home/david/Literatur/rudolph2016/rudolph2016.bib
@inproceedings{rudolph2016,
address = {Cham},
author = {Rudolph, Stefan and von Mammen, Sebastian and Jungbluth, Johannes and H{\"a}hner, J{\"o}rg},
booktitle = {Applications of Evolutionary Computation},
editor = {Squillero, Giovanni and Burelli, Paolo},
isbn = {978-3-319-31204-0},
pages = {669--681},
publisher = {Springer International Publishing},
title = {Design and Evaluation of an Extended Learning Classifier-Based StarCraft Micro AI},
year = {2016},
}


/home/david/Literatur/bull2003/bull2003.bib
@inproceedings{bull2003,
author={Larry Bull and Jacob Hurst},
booktitle={The 2003 Congress on Evolutionary Computation, 2003. {CEC} '03.},
title={A neural learning classifier system with self-adaptive constructivism},
year={2003},
volume={2},
number={},
pages={991--997 Vol. 2},
}


/home/david/Literatur/santu2014/santu2014.bib
@inproceedings{santu2014,
  author = {Shubhra K. K. Santu and Md. Mustafizur Rahman and Md. Monirul Islam and Kazuyuki Murase},
  booktitle = {2014 {IEEE} Congress on Evolutionary Computation ({CEC})},
  doi = {10.1109/CEC.2014.6900388},
  issn = {1941-0026},
  month = {July},
  pages = {1666--1673},
  title = {Towards better generalization in {Pittsburgh} learning classifier systems},
  year = {2014},
}
  volume = {},
  number = {},
  keywords = {evolutionary computation;generalisation (artificial intelligence);learning (artificial intelligence);pattern classification;Pittsburgh learning classifier systems;generalization ability;classification task;EDARIC evolutionary system;evolutionary machine learning;Pittsburgh approach;destructive approach;intelligent deletion mechanism;ensemble system;rule post-processing step;parameter tuning;Accuracy;Sociology;Statistics;Wheels;Standards;Training;Biological cells},


/home/david/Literatur/wadler1990/wadler1990.bib
@inproceedings{wadler1990,
 author = {Wadler, Philip},
 booktitle = {Proceedings of the 1990 {ACM} Conference on {LISP} and Functional Programming},
 pages = {61--78},
 publisher = {{ACM}},
 title = {Comprehending Monads},
 year = {1990},
}
 address = {New York, NY, USA},
 series = {LFP '90},
 conference location = {Nice, France},
 isbn = {0-89791-368-X},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/91556.91592},
 doi = {10.1145/91556.91592},
 acmid = {91592},


/home/david/Literatur/butz2004e/butz2004e.bib
@techreport{butz2004e,
  author = {Martin V. Butz and Martin Pelikan},
  institution = {Illinois Genetic Algorithms Laboratory, University of Illinois at Urbana-Champaign},
  month = {Jul},
  number = {2004033},
  title = {Bounding the Population Size to Ensure Niche Support in XCS},
  year = {2004},
}


/home/david/Literatur/dasilva2020/dasilva2020.bib
﻿@article{dasilva2019,
  author={Da Silva, Felipe Leno and Warnell, Garrett and Costa, Anna Helena Reali and Stone, Peter},
  title={Agents teaching agents: a survey on inter-agent transfer learning},
  journal={Autonomous Agents and Multi-Agent Systems},
  year={2019},
  month={Dec},
  day={09},
  volume={34},
  number={1},
  pages={9},
  issn={1573-7454},
  doi={10.1007/s10458-019-09430-0},
  url={https://doi.org/10.1007/s10458-019-09430-0},
}
  abstract={While recent work in reinforcement learning (RL) has led to agents capable of solving increasingly complex tasks, the issue of high sample complexity is still a major concern. This issue has motivated the development of additional techniques that augment RL methods in an attempt to increase task learning speed. In particular, inter-agent teaching---endowing agents with the ability to respond to instructions from others---has been responsible for many of these developments. RL agents that can leverage instruction from a more competent teacher have been shown to be able to learn tasks significantly faster than agents that cannot take advantage of such instruction. That said, the inter-agent teaching paradigm presents many new challenges due to, among other factors, differences between the agents involved in the teaching interaction. As a result, many inter-agent teaching methods work only in restricted settings and have proven difficult to generalize to new domains or scenarios. In this article, we propose two frameworks that provide a comprehensive view of the challenges associated with inter-agent teaching. We highlight state-of-the-art solutions, open problems, prospective applications, and argue that new research in this area should be developed in the context of the proposed frameworks.},


/home/david/Literatur/patternsynonyms.bib
@online{patternsynonyms,
  author = {{GHC Team}},
  title = {Flexible instances},
  titleaddon = {in {Glasgow Haskell Compiler} Users Guide},
  url = {https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/glasgow_exts.html#pattern-synonyms},
  urldate = {2017-07-13},
}
  year = {2015},


/home/david/Literatur/justesen2017/justesen2017.bib
@inproceedings{justesen2017,
  ISSN={2325-4289},
  author={Niels Justesen and Sebastian Risi},
  booktitle={2017 IEEE Conference on Computational Intelligence and Games (CIG)},
  doi={10.1109/CIG.2017.8080430},
  month={Aug},
  number={},
  pages={162-169},
  title={Learning macromanagement in {StarCraft} from replays using deep learning},
  volume={},
  year={2017},
}
keywords={computer games;learning (artificial intelligence);neural nets;realtime macromanagement tasks;state-action pairs;StarCraft strategy game;hard-coded strategies;strong StarCraft bots;deep reinforcement learning;deep network approach;hand-crafted strategies;Terran bot;open source StarCraft bot;UAlbertaBot;trained network;neural networks;game replays;macromanagement decisions;artificial intelligence techniques;deep learning;Games;Machine learning;Neural networks;Hidden Markov models;Real-time systems},


/home/david/Literatur/partin2018.bib
@online{partin2018,
  author  = {Will Partin},
  title   = {‘StarCraft II’: How Blizzard Brought the King of Esports Back From the Dead},
  month   = {July},
  year    = {2018},
  url     = {https://variety.com/2018/gaming/features/starcraft-ii-esports-history-1202873246/},
  urldate = {2019-08-13},
  lastaccessed = {2019-08-13},
}


/home/david/Literatur/drugowitsch2006/drugowitsch2006.bib
@techreport{drugowitsch2006,
  author = {Jan Drugowitsch and Alwyn M. Barry},
  institution = {University of Bath, Department of Computer Science},
  month = {Jan},
  number = {CSBU-2006-03},
  title = {Towards convergence of learning classifier systems value iteration},
  year = {2006},
}
  Computer Science Technical Reports
  url = {https://drugowitschlab.hms.harvard.edu/files/drugowitschlab/files/tr2006b.pdf},


/home/david/Literatur/iqbal2015/iqbal2015.bib
@article{iqbal2015,
  author    = {Iqbal, Muhammad and Browne, Will N. and Zhang, Mengjie},
  title     = {Improving genetic search in {XCS}-based classifier systems through understanding the evolvability of classifier rules},
  journal   = {Soft Computing},
  year      = {2015},
  volume    = {19},
  number    = {7},
  pages     = {1863--1880},
  publisher = {Springer},
  address   = {Berlin, Heidelberg},
}
  month   = {Jul},
  day     = {01},
  issn    = {1433-7479},
  url       = {https://doi.org/10.1007/s00500-014-1369-7},
  doi       = {10.1007/s00500-014-1369-7},
  biburl    = {https://dblp.org/rec/bib/journals/soco/0001BZ15},
  bibsource = {dblp computer science bibliography, https://dblp.org}


/home/david/Literatur/orriols-puig2009/orriols-puig2009.bib
@article{orriols-puig2009,
  author = {Albert Orriols-Puig and Ester Bernado-Mansilla and David E. Goldberg and Kumara Sastry and Pier Luca Lanzi},
  journal = {IEEE Transactions on Evolutionary Computation},
  month = {Oct},
  number = {5},
  pages = {1093--1119},
  title = {Facetwise Analysis of XCS for Problems With Class Imbalances},
  volume = {13},
  year = {2009},
}
  issn = {1089-778X},
  doi = {10.1109/TEVC.2009.2019829},
  keywords = {genetic algorithms;learning systems;pattern classification;Michigan-style learning classifier systems;online machine learning;design decomposition;XCS facetwise analysis;class imbalance problem;Machine learning;Genetic algorithms;Government;Laboratories;Genetic engineering;Systems engineering and theory;Learning systems;Failure analysis;Guidelines;Military computing;Class imbalance problem;facetwise modeling;genetic algorithms;learning classifier systems;patchquilt integration},


/home/david/Literatur/orriols-puig2010/orriols-puig2010.bib
@inproceedings{orriols-puig2010,
  address = {New York, NY, USA},
  author = {Orriols-Puig, Albert and Llor\`{a}, Xavier and Goldberg, David E.},
  booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
  pages = {1023--1030},
  publisher = {ACM},
  series = {GECCO '10},
  title = {How XCS Deals with Rarities in Domains with Continuous Attributes},
  year = {2010},
}
  location = {Portland, Oregon, USA},
  keywords = {class imbalance problem, genetic-based machine learning, learning classifier systems, small disjuncts},
  numpages = {8},
  acmid = {1830670},
  doi = {10.1145/1830483.1830670},
  isbn = {978-1-4503-0072-8},
  url = {http://doi.acm.org/10.1145/1830483.1830670},


/home/david/Literatur/sommer2016b/sommer2016b.bib
@incollection{sommer2016b,
  author    = {Matthias Sommer and Anthony Stein and J{\"o}rg H{\"a}hner},
  title     = {Ensemble Time Series Forecasting with XCSF},
  booktitle = {2016 IEEE 10th International Conference on Self-Adaptive and Self-Organizing Systems (SASO), 12-16 Sept. 2016, Augsburg, Germany},
  editor    = {Giacomo Cabri and Gauthier Picard and Niranjan Suri},
  isbn      = {9781509035342},
  doi       = {10.1109/saso.2016.25},
  year      = {2016},
}


/home/david/Literatur/butz2001/butz2001.bib
@inproceedings{butz2001,
  author       = {Martin V. Butz and Stewart W. Wilson},
  booksubtitle = {Paris, France, September 15--16, 2000, Revised Papers},
  booktitle    = {Advances in Learning Classifier Systems, Third International Workshop, {IWLCS} 2000},
  pages        = {253--272},
  publisher    = {Springer},
  title        = {An Algorithmic Description of {XCS}},
  year         = {2001},
  editor       = {Pier Luca Lanzi and Wolfgang Stolzmann and Stewart W. Wilson},
}
  address      = {Berlin, Heidelberg},
  doi       = {10.1007/3-540-44640-0_15},
  crossref  = {DBLP:conf/iwlcs/2000},
  url       = {https://doi.org/10.1007/3-540-44640-0_15},
  biburl    = {https://dblp.org/rec/bib/conf/iwlcs/ButzW00},
  bibsource = {dblp computer science bibliography, https://dblp.org}

  proceedings DBLP:conf/iwlcs/2000,
  title     = {Advances in Learning Classifier Systems, Third International Workshop,
               {IWLCS} 2000, Paris, France, September 15-16, 2000, Revised Papers},
  series    = {Lecture Notes in Computer Science},
  volume    = {1996},
  publisher = {Springer},
  year      = {2001},
  url       = {https://doi.org/10.1007/3-540-44640-0},
  doi       = {10.1007/3-540-44640-0},
  isbn      = {3-540-42437-7},
  timestamp = {Wed, 24 May 2017 15:40:41 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/iwlcs/2000},
  bibsource = {dblp computer science bibliography, https://dblp.org}


/home/david/Literatur/vinyals2017/vinyals2017.bib
@article{vinyals2017,
  author    = {Oriol Vinyals and Timo Ewalds and Sergey Bartunov and Petko Georgiev and Alexander Sasha Vezhnevets and Michelle Yeo and Alireza Makhzani and Heinrich K{\"{u}}ttler and John Agapiou and Julian Schrittwieser and John Quan and Stephen Gaffney and Stig Petersen and Karen Simonyan and Tom Schaul and Hado van Hasselt and David Silver and Timothy P. Lillicrap and Kevin Calderone and Paul Keet and Anthony Brunasso and David Lawrence and Anders Ekermo and Jacob Repp and Rodney Tsing},
  title     = {StarCraft {II:} {A} New Challenge for Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1708.04782},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.04782},
  archivePrefix = {arXiv},
  eprint    = {1708.04782},
  timestamp = {Mon, 13 Aug 2018 16:47:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-04782},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



/home/david/Literatur/pätzel2018/pätzel2018.bib
@inproceedings{pätzel2018,
  address = {New York, NY, USA},
  author = {P\"{a}tzel, David and H\"{a}hner, J\"{o}rg},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  location = {Kyoto, Japan},
  pages = {1434--1441},
  publisher = {ACM},
  series = {GECCO '18},
  title = {An Algebraic Description of XCS},
  year = {2018},
}
  acmid = {3208248},
  doi = {10.1145/3205651.3208248},
  isbn = {978-1-4503-5764-7},
  url = {http://doi.acm.org/10.1145/3205651.3208248},
  keywords = {algebra, extended classifier system, formalisation, functional programming, learning classifier system},
  numpages = {8},


/home/david/Literatur/gloss.bib
@online{gloss,
  author = {Lippmeier, Ben},
  title = {The \emph{gloss} {Haskell} package},
  year = {2017},
  url = {https://hackage.haskell.org/package/gloss},
  urldate = {2017-07-20}
}


/home/david/Literatur/hamagami2006/hamagami2006.bib
@inproceedings{hamagami2006,
 author={Tomoki Hamagami and Takashi Shibuya and Shingo Shimada},
 booktitle={2006 {IEEE} International Conference on Systems, Man and Cybernetics},
 title={Complex-Valued Reinforcement Learning},
 year={2006},
 volume={5},
 number={},
 pages={4175--4179},
} 


/home/david/Literatur/stone2003/stone2003.bib
@article{stone2003,
  address = {Cambridge, MA, USA},
  author = {Stone, Christopher and Bull, Larry},
  journal = {Evolutionary Computation},
  number = {3},
  month = {Sep},
  pages = {299--336},
  publisher = {MIT Press},
  title = {For Real! {XCS} with Continuous-Valued Inputs},
  volume = {11},
  year = {2003},
}
  eprint = {https://doi.org/10.1162/106365603322365315} ,
  doi = {10.1162/106365603322365315},
  url = {https://doi.org/10.1162/106365603322365315},
  abstract = { Many real-world problems are not conveniently expressed using the ternary representation typically used by Learning Classifier Systems and for such problems an interval-based representation is preferable. We analyse two interval-based representations recently proposed for XCS, together with their associated operators and find evidence of considerable representational and operator bias. We propose a new interval-based representation that is more straightforward than the previous ones and analyse its bias. The representations presented and their analysis are also applicable to other Learning Classifier System architectures. We discuss limitations of the real multiplexer problem, a benchmark problem used for Learning Classifier Systems that have a continuous-valued representation, and propose a new test problem, the checkerboard problem, that matches many classes of real-world problem more closely than the real multiplexer. Representations and operators are compared using both the real multiplexer and checkerboard problems and we find that representational, operator and sampling bias all affect the performance of XCS in continuous-valued environments. },
  keywords = {representational bias, continuous-valued input, checkerboard problem, learning classifier system, real multiplexer problem, operator bias, XCS, sampling bias, interval representation, real input}


/home/david/Literatur/butz2005/butz2005.bib
@inbook{butz2005,
  address = {Berlin, Heidelberg},
  author = {Butz, Martin V. and Goldberg, David E. and Luca Lanzi, Pier},
  bookTitle = {Foundations of Learning Classifier Systems},
  editor = {Bull, Larry and Kovacs, Tim},
  pages = {91--125},
  publisher = {Springer Berlin Heidelberg},
  title = {Computational Complexity of the XCS Classifier System},
  year = {2005},
}
  abstract = {Learning classifier systems (LCSs) are online-generalizing rule-based learning systems that use evolutionary computation techniques to evolve an optimal set of rules, that is, a population of classifiers (1; 2). LCSs tackle both single-step classification problems and multi-step reinforcement learning (RL) problems. Although the LCS proposal dates back over twenty years ago, there has been hardly any theory regarding convergence, computational effort, problem instances, etc. Successful applications seemed to rather rely on a ``black art'' of correct parameter settings, supported by powerful computers, than on actual insight.},
  isbn = {978-3-540-32396-9},
  doi = {10.1007/11319122_5},
  url = {https://doi.org/10.1007/11319122_5}


/home/david/Literatur/mcbride2008/mcbride2008.bib
@article{mcbride2008,
  title = {Applicative programming with effects},
  volume = {18},
  DOI = {10.1017/S0956796807006326},
  number = {1},
  journal = {Journal of Functional Programming},
  publisher = {Cambridge University Press},
  author = {McBride, Conor and Paterson, Ross},
  year = {2008},
  pages = {1–13},
}


/home/david/Literatur/extended-reals.bib
@online{extended-reals,
  author = {Sakai, Masahiro},
  title = {The \emph{extended-reals} {Haskell} package},
  year = {2016},
  url = {https://hackage.haskell.org/package/extended-reals},
  urldate = {2017-07-14}
}


/home/david/Literatur/butz2002c/butz2002c.bib
@book{butz2002c,
  author = {Martin V. Butz},
  title = {Anticipatory Learning Classifier Systems},
  publisher = {Springer US},
  isbn = {978-1-4615-0891-5},
  series = {Genetic Algorithms and Evolutionary Computation},
  volume = {4},
  issn = {1568-2587},
  year = {2002},
}


/home/david/Literatur/butz2004c/butz2004c.bib
@techreport{butz2004c,
  author = {Martin V. Butz},
  institution = {Illinois Genetic Algorithms Laboratory, University of Illinois at Urbana-Champaign},
  month = {Feb},
  number = {UIUCDCS-R-2004-2474},
  title = {Rule-based Evolutionary Online Learning Systems: Learning Bounds, Classification, and Prediction},
  year = {2004},
}


/home/david/Literatur/borna2019/borna2019.bib
@article{borna2019,
  title={Customer satisfaction prediction with Michigan-style learning classifier system},
  author={Borna, Keivan and Hoseini, Shokoofeh and Aghaei, Mohammad Ali Mehdi},
  journal={SN Applied Sciences},
  volume={1},
  number={11},
  pages={1450--1453},
  issn={2523-3971},
  doi={https://doi.org/10.1007/s42452-019-1493-1},
  year={2019},
  month={10},
  day={21},
  publisher={Springer},
}


/home/david/Literatur/tatsumi2019c/tatsumi2019c.bib
@article{tatsumi2019c,
  title={Acquiring Classifiers for Bipolarized Reward by {XCS} in a Continuous Reward Environment},
  author={Takato Tatsumi and Keiki Takadama},
  journal={{SICE} Journal of Control, Measurement, and System Integration},
  volume={12},
  number={3},
  pages={124--132},
  year={2019},
  doi={10.9746/jcmsi.12.124},
}


/home/david/Literatur/miller1995/miller1995.bib
@article{miller1995,
  author = {Miller, Brad L. and Goldberg, David E. and others},
  journal = {Complex systems},
  number = {3},
  pages = {193--212},
  publisher = {Champaign, IL, {USA}. Complex Systems Publications, Inc.},
  title = {Genetic algorithms, tournament selection, and the effects of noise},
  volume = {9},
  year = {1995},
}


/home/david/Literatur/wilson1994/wilson1994.bib
@article{wilson1994,
  author = {Wilson, Stewart W.},
  title = {{ZCS}: A Zeroth Level Classifier System},
  journal = {Evolutionary Computation},
  volume = {2},
  number = {1},
  pages = {1-18},
  year = {1994},
  doi = {10.1162/evco.1994.2.1.1},
  URL = {https://doi.org/10.1162/evco.1994.2.1.1},
  eprint = {https://doi.org/10.1162/evco.1994.2.1.1} ,
}
  abstract = { A basic classifier system, ZCS, is presented that keeps much of Holland's original framework but simplifies it to increase understandability and performance. ZCS's relation to Q-learning is brought out, and their performances compared in environments of two difficulty levels. Extensions to ZCS are proposed for temporary memory, better action selection, more efficient use of the genetic algorithm, and more general classifier representation. }




/home/david/Literatur/patternsynonymsticket.bib
@online{patternsynonyms-ticket,
  author = {{GHC Team}},
  title = {{GHC} feature request: Allow inline pragmas on pattern synonyms},
  year = {2017},
  url = {https://ghc.haskell.org/trac/ghc/ticket/12178},
  urldate = {2017-07-20}
}


/home/david/Literatur/kovacs2001/kovacs2001.bib
@inproceedings{kovacs2001,
  address = {Berlin, Heidelberg},
  author = {Kovacs, Tim and Kerber, Manfred},
  booktitle = {Advances in Learning Classifier Systems},
  editor = {Lanzi, Pier Luca and Stolzmann, Wolfgang and Wilson, Stewart W.},
  pages = {80--99},
  publisher = {Springer Berlin Heidelberg},
  title = {What Makes a Problem Hard for XCS?},
  year = {2001},
}
  isbn = {978-3-540-44640-8},
  abstract = {Despite two decades of work learning classifier systems researchers have had relatively little to say on the subject of what makes a problem difficult for a classifier system. Wilson's accuracy-based XCS, a promising and increasingly popular classifier system, is, we feel, the natural first choice of classifier system with which to address this issue. To make the task more tractable we limit our considerations to a restricted, but very important, class of problems. Most significantly, we consider only single step reinforcement learning problems and the use of the standard binary/ternary classifier systems language. In addition to distinguishing several dimensions of problem complexity for XCS, we consider their interactions, identify bounding cases of difficulty, and consider complexity metrics for XCS. Based on these results we suggest a simple template for ternary single step test suites to more comprehensively evaluate classifier systems.},



/home/david/Literatur/felter2014/felter2014.bib
@inproceedings{felter2015,
  ISSN = {},
  author = {Wes Felter and Alexandre Ferreira and Ram Rajamony and Juan Rubio},
  booktitle = {2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  doi = {10.1109/ISPASS.2015.7095802},
  month = {March},
  number = {},
  pages = {171-172},
  title = {An updated performance comparison of virtual machines and Linux containers},
  volume = {},
  year = {2015},
}
  keywords = {cloud computing;Linux;virtual machines;virtual machines;Linux containers;cloud computing;KVM;Docker;representative hypervisor;container manager;cloud architectures;Containers;Servers;Linux;Virtual machining;Random access memory;Hardware;Throughput},


/home/david/Literatur/mellor2009/mellor2009.bib
@inproceedings{mellor2009,
 acmid = {1570071},
 address = {New York, NY, USA},
 author = {Mellor, Drew and Nicklin, Steven P.},
 booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
 doi = {10.1145/1569901.1570071},
 isbn = {978-1-60558-325-9},
 keywords = {efficient matching, lcs, learning classifier systems, xcs},
 location = {Montreal, Qu\&\#233;bec, Canada},
 numpages = {8},
 pages = {1267--1274},
 publisher = {ACM},
 series = {GECCO '09},
 title = {A Population-based Approach to Finding the Matchset of a Learning Classifier System Efficiently},
 url = {http://doi.acm.org/10.1145/1569901.1570071},
 year = {2009},
} 


/home/david/Literatur/edakunni2011/edakunni2011.bib
@inproceedings{edakunni2011,
  address = {New York, NY, USA},
  author    = {Narayanan Unny Edakunni and Gavin Brown and Tim Kovacs},
  booktitle = {13th Annual Genetic and Evolutionary Computation Conference, {GECCO} 2011, Proceedings, Dublin, Ireland, July 12-16, 2011},
  crossref  = {gecco2011},
  pages     = {1267--1274},
  publisher = {{ACM}},
  title     = {Online, {GA} based mixture of experts: a probabilistic model of {UCS}},
  year      = {2011},
}
  url       = {https://doi.org/10.1145/2001576.2001747},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/gecco/EdakunniBK11},
  doi       = {10.1145/2001576.2001747},
  timestamp = {Tue, 06 Nov 2018 11:06:39 +0100},
@proceedings{gecco2011,
  editor    = {Natalio Krasnogor and Pier Luca Lanzi},
  title     = {13th Annual Genetic and Evolutionary Computation Conference, {GECCO} 2011, Proceedings, Dublin, Ireland, July 12-16, 2011},
  publisher = {{ACM}},
  year      = {2011},
}
  isbn      = {978-1-4503-0557-0},
  timestamp = {Thu, 14 Jul 2011 22:06:06 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/gecco/2011},
  bibsource = {dblp computer science bibliography, https://dblp.org}


/home/david/Literatur/krawczyk2017/krawczyk2017.bib
@article{krawczyk2017,
  author = {Bartosz Krawczyk and Leandro L. Minku and João Gama and Jerzy Stefanowski and Michał Woźniak},
  doi = {https://doi.org/10.1016/j.inffus.2017.02.004},
  issn = {1566-2535},
  journal = {Information Fusion},
  keywords = {Ensemble learning, Data streams, Concept drift, Online learning, Non-stationary environments},
  pages = {132 - 156},
  title = {Ensemble learning for data stream analysis: A survey},
  url = {http://www.sciencedirect.com/science/article/pii/S1566253516302329},
  volume = {37},
  year = {2017},
}
  abstract = {In many applications of information systems learning algorithms have to act in dynamic environments where data are collected in the form of transient data streams. Compared to static data mining, processing streams imposes new computational requirements for algorithms to incrementally process incoming examples while using limited memory and time. Furthermore, due to the non-stationary characteristics of streaming data, prediction models are often also required to adapt to concept drifts. Out of several new proposed stream algorithms, ensembles play an important role, in particular for non-stationary environments. This paper surveys research on ensembles for data stream classification as well as regression tasks. Besides presenting a comprehensive spectrum of ensemble approaches for data streams, we also discuss advanced learning concepts such as imbalanced data streams, novelty detection, active and semi-supervised learning, complex data representations and structured outputs. The paper concludes with a discussion of open research problems and lines of future research.}


/home/david/Literatur/ontanon2013/ontanon2013.bib
@article{ontanon2013,
  ISSN = {1943-068X},
  author = {Santiago Ontañón and Gabriel Synnaeve and Alberto Uriarte and Florian Richoux and David Churchill and Mike Preuss},
  doi = {10.1109/TCIAIG.2013.2286295},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  month = {Dec},
  number = {4},
  pages = {293-311},
  title = {A Survey of Real-Time Strategy Game {AI} Research and Competition in {StarCraft}},
  volume = {5},
  year = {2013},
}
keywords = {artificial intelligence;computer games;real-time systems;real-time strategy game AI research;RTS games;StarCraft AI competitions;Games;Artificial intelligence;Planning;Cognition;Buildings;Real-time systems;Uncertainty;Game AI;real-time strategy;review1;StarCraft},


/home/david/Literatur/javadoc.bib
@online{javadoc,
  author = {Oracle},
  title = {Javadoc Tool},
  url = {http://www.oracle.com/technetwork/java/javase/documentation/index-jsp-135444.html},
  urldate = {2017-07-11}
}
  year = {2014},


/home/david/Literatur/lam2010/lam2010.bib
@article{lam2010,
  author={Albert Y. S. Lam and Victor O. K. Li},
  journal={{IEEE} Transactions on Evolutionary Computation},
  title={Chemical-reaction-inspired metaheuristic for optimization},
  year={2010},
  volume={14},
  number={3},
  pages={381--399},
  doi={10.1109/TEVC.2009.2033580},
  ISSN={1941-0026},
  month={June},
}
  keywords={chemical reactions;combinatorial mathematics;computational complexity;optimisation;chemical reaction optimization;metaheuristic;low energy stable state;nondeterministic polynomial-time hard combinatorial optimization;no-free-lunch theorem;Chemicals;Optimization methods;NP-hard problem;Cost function;Benchmark testing;Power generation economics;Interference;Electrical engineering;Buildings;Cities and towns;Chemical reaction;metaheuristics;nature-inspired algorithms;optimization methods},


/home/david/Literatur/hughes1989/hughes1989.bib
@article{hughes1989,
  author = {John Hughes},
  title = {Why Functional Programming Matters},
  journal = {Computer Journal},
  volume = {32},
  number = {2},
  pages = {98--107},
  year = {1989}
}


/home/david/Literatur/liang2019/liang2019.bib
@inproceedings{liang2019,
    author = {Megan Liang and Gabrielle Palado and Will N. Browne},
    booktitle = {2019 International Conference on Image and Vision Computing New Zealand ({IVCNZ})},
    title = {Identifying Simple Shapes to Classify the Big Picture},
    year = {2019},
    volume = {},
    number = {},
    pages = {1--6},
    keywords = {computer vision;evolutionary computation;feature extraction;image classification;learning (artificial intelligence);neural nets;shape recognition;Deep Artificial Neural Networks;visual classification problems;impediment;transparency;LCSs;Evolutionary Computation technique;human-readable rules;pixel images;feature extraction;object classification;decision boundaries;classification decisions;explainable artificial intelligence;computer vision;DNN-LCS system;shape identification;Object Recognition;Learning Classifier Systems},
    doi = {10.1109/IVCNZ48456.2019.8960989},
    issn = {2151-2191},
    month = {Dec},
}


/home/david/Literatur/wada2005/wada2005.bib
@inbook{wada2005,
  address = {Berlin, Heidelberg},
  author = {Wada, Atsushi and Takadama, Keiki and Shimohara, Katsunori and Katai, Osamu},
  booktitle = {Foundations of Learning Classifier Systems},
  editor = {Bull, Larry and Kovacs, Tim},
  pages = {285--304},
  publisher = {Springer Berlin Heidelberg},
  title = {Learning Classifier System with Convergence and Generalization},
  year = {2005},
}
  isbn = {978-3-540-32396-9},
  doi = {10.1007/11319122_11},
  url = {https://doi.org/10.1007/11319122_11},
  abstract = {Learning Classifier Systems (LCSs) are rule-based systems whose rules are named classifiers. The original LCS was introduced by Holland [1, 2], and was intended to be a framework to study learning in condition-action rules. It included the distinctive features of a generalization mechanism in rule conditions and a rule discovery mechanism using genetic algorithms (GAs) [3]. Later, this original LCS was revised to its ``standard form''[4], which produced many variants [5--8].},


/home/david/Literatur/haddock.bib
@online{haddock,
  author = {Marlow, Simon},
  title = {Haddock: A {Haskell} Documentation Tool},
  url = {https://www.haskell.org/haddock/},
  urldate = {2017-07-11}
}
  year = {2014},


/home/david/Literatur/doerr2019/doerr2019.bib
@inproceedings{doerr2019,
  address = {New York, NY, USA},
  archivePrefix = {arXiv},
  author = {Doerr, Benjamin and Doerr, Carola and Lengler, Johannes},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  doi = {10.1145/3321707.3321733},
  eprint = {1902.02588},
  isbn = {9781450361118},
  location = {Prague, Czech Republic},
  numpages = {9},
  pages = {1479–1487},
  primaryClass = {cs.NE},
  publisher = {Association for Computing Machinery},
  series = {GECCO ’19},
  title = {Self-adjusting mutation rates with provably optimal success rules},
  url = {https://doi.org/10.1145/3321707.3321733},
  year = {2019},
}


/home/david/Literatur/turing1937/turing1937.bib
@article{turing1937,
  title = {Computability and λ-definability},
  volume = {2},
  number = {4},
  journal = {Journal of Symbolic Logic},
  publisher = {Cambridge University Press},
  author = {Alan M. Turing},
  year = {1937},
  pages = {153--163},
}
  DOI = {10.2307/2268280},


/home/david/Literatur/russel2009/russel2009.bib
@book{russel2009,
  address = {USA},
  author = {Russell, Stuart and Norvig, Peter},
  edition = {3rd},
  isbn = {0136042597},
  publisher = {Prentice Hall Press},
  title = {Artificial Intelligence: A Modern Approach},
  year = {2009},
}


/home/david/Literatur/dixon2002/dixon2002.bib
@InProceedings{dixon2002,
  address = "Berlin, Heidelberg",
  author = "Dixon, Phillip William and Corne, David W. and Oates, Martin John",
  booktitle = "Advances in Learning Classifier Systems",
  editor = "Lanzi, Pier Luca and Stolzmann, Wolfgang and Wilson, Stewart W.",
  isbn = "978-3-540-48104-1",
  pages = "133--150",
  publisher = "Springer Berlin Heidelberg",
  title = "A Preliminary Investigation of Modified XCS as a Generic Data Mining Tool",
  year = "2002",
}



/home/david/Literatur/algorithmwatch2019.bib
@online{algorithmwatch2019,
  label   = {AW},
  author  = {Nicolas Kayser-Bril},
  title   = {Austria’s employment agency rolls out discriminatory algorithm, sees no problem},
  day     = {6},
  month   = {October},
  year    = {2019},
  url     = {https://algorithmwatch.org/en/story/austrias-employment-agency-ams-rolls-out-discriminatory-algorithm/},
  urldate = {2020-06-25},
  lastaccessed = {2020-06-25},
}


/home/david/Literatur/liu2019b/liu2019b.bib
@inproceedings{liu2019b,
    author = {Liu, Yi and Browne, Will N. and Xue, Bing},
    title = {Absumption to Complement Subsumption in Learning Classifier Systems},
    year = {2019},
    isbn = {9781450361118},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3321707.3321719},
    doi = {10.1145/3321707.3321719},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
    pages = {410--418},
    numpages = {9},
    keywords = {learning classifier system, absumption, subsumption},
    location = {Prague, Czech Republic},
    series = {GECCO ’19},
}


/home/david/Literatur/kairouz2019/kairouz2019.bib
@misc{kairouz2019,
    title={Advances and Open Problems in Federated Learning},
    author={Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Keith Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D'Oliveira and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konečný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Mariana Raykova and Hang Qi and Daniel Ramage and Ramesh Raskar and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
    year={2019},
    eprint={1912.04977},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
}


/home/david/Literatur/rehman2019/rehman2019.bib
@article{rehman2019,
    author = {Asadul Rehman, Hafiz and Iqbal, Muhammad and Younas, Irfan and Bashir, Maryam},
    title = {Learning Regular Expressions Using {XCS}-Based Classifier System},
    journal = {International Journal of Pattern Recognition and Artificial Intelligence},
    volume = {0},
    number = {0},
    pages = {2051011},
    year = {0},
    doi = {10.1142/S0218001420510118},
    url = {https://doi.org/10.1142/S0218001420510118},
    eprint = {https://doi.org/10.1142/S0218001420510118},
}
abstract = { Evolutionary machine learning research aims to develop classifier systems that can solve complex and hard tasks. This paper addresses the problem of inferring a regular expression from a given set of strings for automating the task of information extraction. To the best of our knowledge, this paper is the first to propose the extension of accuracy-based classifier system XCS to learn the regular expressions for text extraction. This new system named as XCSREA includes tree-like code fragments to learn regular expressions. The genetic algorithm in action sets uses two-point crossover with uniform mutation and Roulette wheel parent selection method. Seven different datasets, each with three different lengths, are used to compare the performance of the proposed model with standard genetic programming (GP) approach. The experimental results demonstrate that XCSREA outperforms standard GP approach when sufficiently large numbers of classifiers are used. }


/home/david/Literatur/wang2019/wang2019.bib
@inproceedings{wang2019,
    author = {Chang Wang and Hao Chen and Chao Yan and Xiaojia Xiang},
    booktitle = {2019 {IEEE} International Conference on Agents ({ICA})},
    title = {Reinforcement Learning with an Extended Classifier System in Zero-sum Markov Games},
    year = {2019},
    volume = {},
    number = {},
    pages = {44--49},
    doi = {10.1109/AGENTS.2019.8929148},
    month = {Oct},
}
    keywords = {function approximation;game theory;genetic algorithms;learning (artificial intelligence);Markov processes;multi-agent systems;optimal policy;RL agent;opponent agent;learning process;reinforcement learning agent;RL algorithm;extended classifier system;zero-sum Markov games;eligibility trace;adversarial soccer game;deterministic policy learner;Games;Markov processes;Genetic algorithms;Task analysis;Prediction algorithms;Learning (artificial intelligence);Game theory;reinforcement learning;learning classifier system;multi-agent system;markov games;eligibility trace},
    issn = {null},


/home/david/Literatur/wilson1996/wilson1996.bib
@inproceedings{wilson1996,
  author        = {Stewart W. Wilson},
  booktitle     = {Machine Learning, Proceedings of the 13th International Conference},
  series        = {ICML '96},
  editor        = {Lorenza Saitta},
  publisher     = {Morgan Kaufmann},
  title         = {Generalization in XCS},
  year          = {1996},
  pages         = {3--11},
}
  isbn          = 1-55860-419-7,
  address       = {Bari, Italy},


/home/david/Literatur/bull2019/bull2019.bib
@misc{bull2019,
    title={Autoencoding with a Learning Classifier System: Initial Results},
    author={Larry Bull},
    year={2019},
    eprint={1907.11554},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}


/home/david/Literatur/bu2020/bu2020.bib
@article{bu2020,
    title = {A convolutional neural-based learning classifier system for detecting database intrusion via insider attack},
    journal = {Information Sciences},
    volume = {512},
    pages = {123--136},
    year = {2020},
    issn = {0020-0255},
    doi = {https://doi.org/10.1016/j.ins.2019.09.055},
    url = {http://www.sciencedirect.com/science/article/pii/S0020025519309004},
    author = {Seok-Jun Bu and Sung-Bae Cho},
    keywords = {Deep learning, Convolutional neural network, Learning classifier system, Database intrusion detection},
    abstract = {Role-based access control (RBAC) in databases provides a valuable level of abstraction to promote security administration at the business enterprise level. With the capacity for adaptation and learning, machine learning algorithms are suitable for modeling normal data access patterns based on large amounts of data and presenting robust statistical models that are not sensitive to user changes. We propose a convolutional neural-based learning classifier system (CN-LCS) that models the role of queries by combining conventional learning classifier system (LCS) with convolutional neural network (CNN) for a database intrusion detection system based on the RBAC mechanism. The combination of modified Pittsburgh-style LCSs for the optimization of feature selection rules and one-dimensional CNNs for modeling and classification in place of traditional rule generation outperforms other machine learning classifiers on a synthetic query dataset. In order to quantitatively compare the inclusion of rule generation and modeling processes in the CN-LCS, we have conducted 10-fold cross-validation tests and analysis through a paired sampled t-test.}
}


/home/david/Literatur/zhuo2019/zhuo2019.bib
@misc{zhuo2019,
    title={Federated Deep Reinforcement Learning},
    author={Hankz Hankui Zhuo and Wenfeng Feng and Yufeng Lin and Qian Xu and Qiang Yang},
    year={2019},
    eprint={1901.08277},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
}


/home/david/Literatur/moschoyiannis2019/moschoyiannis2019.bib
@inproceedings{moschoyiannis2019,
  author    = {Sotiris Moschoyiannis and Vasily Shcherbinin},
  editor    = {Ahmet Soylu and Sotiris Moschoyiannis and Guido Governatori and Mantas Simkus and Petros Stefaneas and Alexander Steen and Adrian Giurca},
  title     = {Fine Tuning Run Parameter Values in Rule-Based Machine Learning},
  booktitle = {Proceedings of the 13th {RuleML+RR} 2019 Doctoral Consortium and Rule Challenge, September 16--19, 2019 (Bolzano, Italy)},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {2438},
  publisher = {CEUR-WS.org},
  year      = {2019},
  url       = {http://ceur-ws.org/Vol-2438/paper9.pdf},
  timestamp = {Wed, 12 Feb 2020 16:44:23 +0100},
  biburl    = {https://dblp.org/rec/conf/ruleml/MoschoyiannisS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


/home/david/Literatur/shrivastava2004/shrivastava2004.bib
@inproceedings{shrivastava2004,
  title={Medians and Beyond: New Aggregation Techniques for Sensor Networks},
  author={Shrivastava, Nisheeth and Buragohain, Chiranjeeb and Agrawal, Divyakant and Suri, Subhash},
  booktitle={Proceedings of the 2nd international conference on Embedded networked sensor systems},
  pages={239--249},
  year={2004},
  organization={ACM}
}


/home/david/Literatur/brent1973/brent1973.bib


/home/david/Literatur/butz2006d/butz2006d.bib
@book{butz2006d,
  address   = {Berlin, Heidelberg},
  author    = {Martin V. Butz},
  publisher = {Springer Berlin Heidelberg},
  series    = {Studies in Fuzziness and Soft Computing},
  title     = {Rule-Based Evolutionary Online Learning Systems - {A} Principled Approach to {LCS} Analysis and Design},
  volume    = {191},
  year      = {2006},
}
  url       = {https://doi.org/10.1007/b104669},
  doi       = {10.1007/b104669},
  isbn      = {978-3-540-25379-2},
  timestamp = {Tue, 16 May 2017 14:01:46 +0200},
  biburl    = {https://dblp.org/rec/bib/books/sp/Butz06},
  bibsource = {dblp computer science bibliography, https://dblp.org}


/home/david/Literatur/sutton1999/sutton1999.bib
@article{sutton1999,
title = {Between {MDPs} and {semi-MDPs}: A framework for temporal abstraction in reinforcement learning},
journal = "Artificial Intelligence",
volume = "112",
number = "1",
pages = "181--211",
year = "1999",
issn = "0004-3702",
doi = "https://doi.org/10.1016/S0004-3702(99)00052-1",
url = "http://www.sciencedirect.com/science/article/pii/S0004370299000521",
author = "Richard S. Sutton and Doina Precup and Satinder Singh",
keywords = "Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes",
}
abstract = "Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.",


/home/david/Literatur/samarakoon2019/samarakoon2019.bib
@ARTICLE{samarakoon2019,
    author = {Sumudu {Samarakoon} and Mehdi {Bennis} and Walid {Saad} and Mérouane {Debbah}},
    doi = {10.1109/TCOMM.2019.2956472},
    issn = {1558-0857},
    journal = {IEEE Transactions on Communications},
    keywords = {Delays;Reliability;Wireless communication;Data models;Resource management;Power demand;Probabilistic logic},
    month = {},
    number = {},
    pages = {1-1},
    title = {Distributed Federated Learning for Ultra-Reliable Low-Latency Vehicular Communications},
    volume = {},
    year = {2019},
}


/home/david/Literatur/drugowitsch2007a/drugowitsch2007a.bib
@inproceedings{drugowitsch2007a,
  address = {New York, NY, USA},
  author = {Drugowitsch, Jan and Barry, Alwyn M.},
  title = {Mixing Independent Classifiers},
  booktitle = {Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation},
  booksubtitle = {London, England, UK, July 7--11, 2007},
  year = {2007},
  pages = {1596--1603},
  publisher = {{ACM}},
  editor    = {Hod Lipson},
}
  keywords = {information fusion, learning classifier systems},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/1276958.1277278},
  doi = {10.1145/1276958.1277278},
  acmid = {1277278},
  series = {GECCO '07},
  isbn = {978-1-59593-697-4},
  conference location = {London, England},
  pages     = {1596--1603},
  year      = {2007},
  crossref  = {DBLP:conf/gecco/2007},
  biburl    = {https://dblp.org/rec/bib/conf/gecco/DrugowitschB07},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  proceedings{DBLP:conf/gecco/2007,
  editor    = {Hod Lipson},
  title     = {Genetic and Evolutionary Computation Conference, {GECCO} 2007, Proceedings,
               London, England, UK, July 7-11, 2007},
  publisher = {{ACM}},
  year      = {2007},
  isbn      = {978-1-59593-697-4},
  timestamp = {Tue, 21 Aug 2007 12:31:32 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/gecco/2007},
  bibsource = {dblp computer science bibliography, https://dblp.org},


/home/david/Literatur/data-interval.bib
@online{data-interval,
  author = {Sakai, Masahiro},
  title = {The \emph{data-interval} {Haskell} package},
  year = {2014},
  url = {https://hackage.haskell.org/package/data-interval},
  urldate = {2017-03-09}
}


/home/david/Literatur/nakata2017a/nakata2017a.bib
@inproceedings{nakata2017a,
  author       = {Masaya Nakata and Will N. Browne and Tomoki Hamagami and Keiki Takadama},
  title        = {Theoretical {XCS} Parameter Settings of Learning Accurate Classifiers},
  booktitle    = {Proceedings of the Genetic and Evolutionary Computation Conference 2017},
  booksubtitle = {Berlin, Germany, July 15--19, 2017},
  pages        = {473--480},
  series       = {GECCO '17},
  year         = {2017},
  editor       = {Peter A. N. Bosman},
  publisher    = {{ACM}},
  address      = {New York, NY, USA},
}
  crossref     = {DBLP:conf/gecco/2017},
  url          = {http://doi.acm.org/10.1145/3071178.3071200},
  doi          = {10.1145/3071178.3071200},
  biburl       = {https://dblp.org/rec/bib/conf/gecco/NakataBHT17},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  proceedings{DBLP:conf/gecco/2017,
  title     = {Proceedings of the Genetic and Evolutionary Computation Conference,
               {GECCO} 2017, Berlin, Germany, July 15-19, 2017},
  year      = {2017},
  url       = {http://doi.acm.org/10.1145/3071178},
  doi       = {10.1145/3071178},
  isbn      = {978-1-4503-4920-8},
  timestamp = {Fri, 14 Jul 2017 14:14:33 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/gecco/2017},
  bibsource = {dblp computer science bibliography, https://dblp.org},


/home/david/Literatur/dutta2020/dutta2020.bib
@article{dutta2020,
    title = {A bi-phased multi-objective genetic algorithm based classifier},
    journal = {Expert Systems with Applications},
    volume = {146},
    pages = {113163},
    year = {2020},
    issn = {0957-4174},
    doi = {https://doi.org/10.1016/j.eswa.2019.113163},
    url = {http://www.sciencedirect.com/science/article/pii/S0957417419308814},
    author = {Dipankar Dutta and Jaya Sil and Paramartha Dutta},
    abstract = {This paper presents a novel Bi-Phased Multi-Objective Genetic Algorithm (BPMOGA) based classification method. It is a Learning Classifier System (LCS) designed for supervised learning tasks. Here we have used Genetic Algorithms (GAs) to discover optimal classifiers from data sets. The objective of the work is to find out a classifier or Complete Rule (CR) which comprises of several Class Specific Rules (CSRs). Phase-I of BPMOGA extracts optimized CSRs in IF−THEN form by following Michigan approach, without considering interaction among the rules. Phase-II of BPMOGA builds optimized CRs from CSRs by following Pittsburgh way. It combines the advantages of both approaches. Extracted CRs help to build CSRs for the next run of phase-I. Hence, phase-I and phase-II are cyclically related, which is one of the uniqueness of BPMOGA. With the help of twenty one benchmark data sets from the University of California at Irvine (UCI) machine learning repository we have compared performance of BPMOGA based classifier with fourteen GA and non-GA based classifiers. Statistical test shows that the performance of the proposed classifier is either superior or comparable to other classifiers.},
}
    keywords = {, Elitist Multi-Objective Genetic Algorithm, Pareto approach, Statistical test},


/home/david/Literatur/drugowitsch2007b/drugowitsch2007b.bib
@phdthesis{drugowitsch2007b,
  author = {Drugowitsch, Jan},
  school = {University of Bath (United Kingdom)},
  title = {Learning classifier systems from first principles: a probabilistic reformulation of learning classifier systems from the perspective of machine learning},
  year = {2007},
}


/home/david/Literatur/dean2008/dean2008.bib
@article{dean2008,
  author    = {Dean, Jeffrey and Ghemawat, Sanjay},
  journal   = {Communications of the ACM},
  number    = {1},
  pages     = {107--113},
  publisher = {ACM},
  title     = {{MapReduce}: Simplified Data Processing on Large Clusters},
  volume    = {51},
  year      = {2008},
}


/home/david/Literatur/orriols-puig2006/orriols-puig2006.bib
@inproceedings{orriols-puig2006,
  address = {New York, NY, USA},
  author = {Orriols-Puig, Albert and Bernad\'{o}-Mansilla, Ester},
  booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
  location = {Seattle, Washington, USA},
  pages = {1561--1568},
  publisher = {ACM},
  series = {GECCO '06},
  title = {Bounding XCS's Parameters for Unbalanced Datasets},
  year = {2006},
}
  doi = {10.1145/1143997.1144250},
  isbn = {1-59593-186-4},
  url = {http://doi.acm.org/10.1145/1143997.1144250},
  numpages = {8},
  acmid = {1144250},
  keywords = {class imbalance, evolutionary computation, genetic algorithms, learning classifier systems, machine learning},


/home/david/Literatur/kovacs2001b/kovacs2001b.bib
@incollection{kovacs2001b,
  address = {San Francisco},
  author = {Tim Kovacs},
  booktitle = {Foundations of Genetic Algorithms 6},
  editor = {Worthy N. Martin and William M. Spears},
  pages = {165--184},
  publisher = {Morgan Kaufmann},
  title = {Towards a Theory of Strong Overgeneral Classifiers},
  year = {2001},
}
  doi = {https://doi.org/10.1016/B978-155860734-7/50092-5},
  isbn = {978-1-55860-734-7},
  url = {http://www.sciencedirect.com/science/article/pii/B9781558607347500925},
  abstract = {We analyse the concept of strong overgeneral rules, the Achilles’ heel of traditional Michigan-style learning classifier systems, using both the traditional strength-based and newer accuracy-based approaches to rule fitness. We argue that different definitions of overgenerality are needed to match the goals of the two approaches, present minimal conditions and environments which will support strong overgeneral rules, demonstrate their dependence on the reward function, and give some indication of what kind of reward functions will avoid them. Finally, we distinguish fit overgeneral rules, show how strength and accuracy-based fitness differ in their response to fit overgenerals and conclude by considering possible extensions to this work.},


/home/david/Literatur/graves2013/2013 Graves - Generating Sequences With Recurrent Neural Networks.bib
@article{graves2013,
  author    = {Alex Graves},
  title     = {Generating Sequences With Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1308.0850},
  year      = {2013},
}
  url       = {http://arxiv.org/abs/1308.0850},
  timestamp = {Wed, 07 Jun 2017 14:40:15 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Graves13},
  bibsource = {dblp computer science bibliography, http://dblp.org}


/home/david/Literatur/bull2000b/bull2000b.bib
@inproceedings{bull2000b,
  address = {Berlin, Heidelberg},
  author = {Bull, Larry},
  booktitle = {Advances in Learning Classifier Systems},
  editor = {Luca Lanzi, Pier and Stolzmann, Wolfgang and Wilson, Stewart W.},
  isbn = {978-3-540-44640-8},
  pages = {29--36},
  publisher = {Springer Berlin Heidelberg},
  title = {Simple Markov Models of the Genetic Algorithm in Classifier Systems: Multi-step Tasks},
  year = {2001},
}
  abstract = {Michigan-style Classifier Systems use Genetic Algorithms to facilitate rule-discovery. This paper presents a simple Markov model of the algorithm in such systems, with the aim of examining the effects of different types of interdependence between niches in multi-step tasks. Using the model it is shown that the existence of, what is here termed, partner rule variance can have significant and detrimental effects on the Genetic Algorithm's expected behaviour. Suggestions are made as to how to reduce these effects, making connections with other recent work in the area.},


/home/david/Literatur/wilson2000/wilson2000.bib
@inproceedings{wilson2000,
  address = {Berlin, Heidelberg},
  author = {Wilson, Stewart W.},
  booktitle = {Learning Classifier Systems},
  editor = {Lanzi, Pier Luca and Stolzmann, Wolfgang and Wilson, Stewart W.},
  isbn = {978-3-540-45027-6},
  pages = {209--219},
  publisher = {Springer Berlin Heidelberg},
  title = {Get Real! XCS with Continuous-Valued Inputs},
  year = {2000},
}
  abstract = {Classifier systems have traditionally taken binary strings as inputs, yet in many real problems such as data inference, the inputs have real components. A modified XCS classifier system is described that learns a non-linear real-vector classification t},


/home/david/Literatur/lhs2tex.bib
@online{lhs2tex,
  author  = {Löh, Andres},
  title   = {{lhs2TeX} Website},
  year    = {2012},
  url     = {https://www.andres-loeh.de/lhs2tex/},
  urldate = {2017-07-20}
}
  author  = {Marlow, Simon},


/home/david/Literatur/orriols-puig2008/orriols-puig2008.bib
@inbook{orriols-puig2008,
  address = {Berlin, Heidelberg},
  author = {Orriols-Puig, Albert and Bernad{\'o}-Mansilla, Ester},
  booktitle = {Learning Classifier Systems in Data Mining},
  editor = {Bull, Larry and Bernad{\'o}-Mansilla, Ester and Holmes, John},
  pages = {123--145},
  publisher = {Springer Berlin Heidelberg},
  title = {Mining Imbalanced Data with Learning Classifier Systems},
  year = {2008},
}
  doi = {10.1007/978-3-540-78979-6_6},
  isbn = {978-3-540-78979-6},
  url = {https://doi.org/10.1007/978-3-540-78979-6_6},
  abstract = {This chapter investigates the capabilities of XCS for mining imbalanced datasets. Initial experiments show that, for moderate and high class imbalances, XCS tends to evolve a large proportion of overgeneral classifiers. Theoretical analyses are developed, deriving an imbalance bound up to which XCS should be able to differentiate between accurate and overgeneral classifiers. Some relevant parameters that have to be properly configured to satisfy the bound for high class imbalances are detected. Configuration guidelines are provided, and an algorithm that automatically tunes these XCS's parameters is presented. Finally, XCS is tested on a large set of real-world problems, appearing to be highly competitive to some of the most well-known machine learning techniques.},



/home/david/Literatur/juliani2019/juliani2019.bib
@article{juliani2019,
  archivePrefix = {arXiv},
  author    = {Arthur Juliani and Ahmed Khalifa and Vincent{-}Pierre Berges and Jonathan Harper and Hunter Henry and Adam Crespi and Julian Togelius and Danny Lange},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-01378},
  eprint    = {1902.01378},
  journal   = {CoRR},
  timestamp = {Tue, 21 May 2019 18:03:39 +0200},
  title     = {Obstacle Tower: {A} Generalization Challenge in Vision, Control, and Planning},
  url       = {http://arxiv.org/abs/1902.01378},
  volume    = {abs/1902.01378},
  year      = {2019},
}


/home/david/Literatur/samvelyan2019/samvelyan2019.bib
@inproceedings{samvelyan2019,
    author = {Samvelyan, Mikayel and Rashid, Tabish and Schroeder de Witt, Christian and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
    title = {The {StarCraft} Multi-Agent Challenge},
    year = {2019},
    isbn = {9781450363099},
    publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
    address = {Richland, SC},
    booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
    pages = {2186--2188},
    numpages = {3},
    keywords = {reinforcement learning, multi-agent learning, starcraft},
    location = {Montreal QC, Canada},
    series = {AAMAS ’19},
}


/home/david/Literatur/butz2001d/butz2001d.bib
@techreport{butz2001d,
  author = {Martin V. Butz and Tim Kovacs and Pier Luca Lanzi and Stewart W. Wilson},
  institution = {Illinois Genetic Algorithms Laboratory, University of Illinois at Urbana-Champaign},
  number = {2001008},
  title = {How XCS Evolves Accurate Classifiers},
  year = {2001},
  month = {Feb},
}


/home/david/Literatur/tadokoro2019/tadokoro2019.bib
@inproceedings{tadokoro2019,
    author={Masakazu Tadokoro and Satoshi Hasegawa and Takato Tatsumi and Hiroyuki Sato and Keiki Takadama},
    booktitle={2019 {IEEE} Congress on Evolutionary Computation ({CEC})},
    title={Knowledge Extraction from {XCSR} Based on Dimensionality Reduction and Deep Generative Models},
    year={2019},
    volume={},
    number={},
    pages={1883--1890},
    keywords={data mining;information retrieval;learning (artificial intelligence);pattern classification;knowledge extraction;XCSR;deep generative model;LCS;ELSDeCS;dimensionality reduction method;DCAXCSR2;VAEXCSR;learning classifier system;interpretable rule representation extraction;encoding learning sampling and decoding classifier system;data mining;handwritten digit classification;Decoding;Dimensionality reduction;Task analysis;Genetic algorithms;Image reconstruction;Encoding;Classification algorithms;data mining;XCS;variational autoencoder;interpretability},
    doi={10.1109/CEC.2019.8790119},
    month={June},
}
    issn={null},


/home/david/Literatur/interval.bib
@online{interval,
  author = {Moutinho, Julien},
  title = {The \emph{interval} {Haskell} package},
  year = {2016},
  url = {https://hackage.haskell.org/package/interval},
  urldate = {2017-07-14}
}


/home/david/Literatur/knox2012/knox2012.bib
@inproceedings{knox2012,
  ISSN = {1944-9437},
  author = {W. Bradley Knox and Peter Stone},
  booktitle = {2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication},
  doi = {10.1109/ROMAN.2012.6343862},
  month = {Sep},
  number = {},
  pages = {878--885},
  title = {Reinforcement learning from human reward: Discounting in episodic tasks},
  volume = {},
  year = {2012},
}


/home/david/Literatur/molina-markham2010/molina-markham2010.bib
@inproceedings{molina-markham2010,
 author = {Molina-Markham, Andr{\'e}s and Shenoy, Prashant and Fu, Kevin and Cecchet, Emmanuel and Irwin, David},
 title = {Private Memoirs of a Smart Meter},
 booktitle = {Proceedings of the 2Nd ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Building},
 series = {BuildSys '10},
 year = {2010},
 isbn = {978-1-4503-0458-0},
 location = {Zurich, Switzerland},
 pages = {61--66},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1878431.1878446},
 doi = {10.1145/1878431.1878446},
 acmid = {1878446},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {privacy, security, smart grid, smart meters},
}


/home/david/Literatur/deepmind2019.bib
@online{deepmind2019,
  label   = {Alp},
  author  = {The {AlphaStar} team},
  title   = {{AlphaStar}: Mastering the Real-Time Strategy Game {StarCraft II}},
  month   = {January},
  year    = {2019},
  url     = {https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii},
  urldate = {2019-08-13},
  lastaccessed = {2019-08-13},
}


/home/david/Literatur/bull2015/bull2015.bib
@article{bull2015,
  author    = {Larry Bull},
  journal   = {Evolutionary Intelligence},
  number    = {2--3},
  pages     = {55--70},
  title     = {A brief history of learning classifier systems: from {CS-1} to {XCS} and its variants},
  volume    = {8},
  year      = {2015},
}
  url       = {https://doi.org/10.1007/s12065-015-0125-y},
  doi       = {10.1007/s12065-015-0125-y},
  timestamp = {Sun, 28 May 2017 13:24:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/evi/Bull15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  month="Sep",
  day="01",
  abstract="The direction set by Wilson's XCS is that modern Learning Classifier Systems can be characterized by their use of rule accuracy as the utility metric for the search algorithm(s) discovering useful rules. Such searching typically takes place within the restricted space of co-active rules for efficiency. This paper gives an overview of the evolution of Learning Classifier Systems up to XCS, and then of some of the subsequent developments of Wilson's algorithm to different types of learning.",
  issn="1864-5917",


/home/david/Literatur/conrady2019/conrady2019.bib
@inproceedings{conrady2019,
    author = {Conrady, Simon and Manuel, Manu and Kreddig, Arne and Stechele, Walter},
    title = {{LCS}-based Automatic Configuration of Approximate Computing Parameters for {FPGA} System Designs},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319619.3326820},
    doi = {10.1145/3319619.3326820},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {1271--1279},
    numpages = {9},
    location = {Prague, Czech Republic},
    series = {GECCO ’19}
}
    keywords = {learning classifier system, approximate computing},


/home/david/Literatur/ACS.bib
@inproceedings{stolzmann1999,
  author    = {Wolfgang Stolzmann},
  editor    = {Pier Luca Lanzi and
               Wolfgang Stolzmann and
               Stewart W. Wilson},
  title     = {An Introduction to Anticipatory Classifier Systems},
  booktitle = {Learning Classifier Systems, From Foundations to Applications},
  series    = {Lecture Notes in Computer Science},
  volume    = {1813},
  pages     = {175--194},
  publisher = {Springer},
  year      = {1999},
}

@inproceedings{butz_stolzmann2001,
  author    = {Martin V. Butz and
               Wolfgang Stolzmann},
  editor    = {Pier Luca Lanzi and
               Wolfgang Stolzmann and
               Stewart W. Wilson},
  title     = {An Algorithmic Description of {ACS2}},
  booktitle = {Advances in Learning Classifier Systems, 4th International Workshop,
               {IWLCS} 2001, San Francisco, CA, USA, July 7-8, 2001, Revised Papers},
  series    = {Lecture Notes in Computer Science},
  volume    = {2321},
  pages     = {211--230},
  publisher = {Springer},
  year      = {2001},
}


/home/david/Literatur/yang2019/yang2019.bib
@article{yang2019,
    address = {New York, NY, USA},
    articleno = {Article 12},
    author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
    doi = {10.1145/3298981},
    issn = {2157-6904},
    issue_date = {February 2019},
    journal = {ACM Trans. Intell. Syst. Technol.},
    keywords = {transfer learning, Federated learning, GDPR},
    month = Jan,
    number = {2},
    numpages = {19},
    publisher = {Association for Computing Machinery},
    title = {Federated Machine Learning: Concept and Applications},
    url = {https://doi.org/10.1145/3298981},
    volume = {10},
    year = {2019},
}


/home/david/Literatur/marin2011/marin2011.bib
@inproceedings{marin2011,
  address = {New York, NY, USA},
  author = {Marin, Didier and Decock, J\'{e}r\'{e}mie and Rigoux, Lionel and Sigaud, Olivier},
  booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
  doi = {10.1145/2001576.2001743},
  isbn = {9781450305570},
  location = {Dublin, Ireland},
  numpages = {8},
  pages = {1235--1242},
  publisher = {Association for Computing Machinery},
  series = {GECCO ’11},
  title = {Learning Cost-Efficient Control Policies with {XCSF}: Generalization Capabilities and Further Improvement},
  url = {https://doi.org/10.1145/2001576.2001743},
  year = {2011},
}
  keywords = {cross-entropy, XCSF, reinforcement learning, control},


/home/david/Literatur/kovacs2002/kovacs2002.bib
@article{kovacs2002,
  author = {Kovacs, Tim},
  journal = {Soft Computing},
  month = {Jun},
  number = {3},
  pages = {171--182},
  title = {What should a classifier system learn and how should we measure it?},
  volume = {6},
  year = {2002},
}
  doi = {10.1007/s005000100114},
  issn = {1432-7643},
  url = {https://doi.org/10.1007/s005000100114},
  abstract = {{\enspace}We consider the issues of how a classifier system should learn to represent a Boolean function, and how we should measure its progress in doing so. We identify four properties which may be desirable of a representation; that it be complete, accurate, minimal and non-overlapping. We distinguish two categories of learning metric, introduce new metrics and evaluate them. We demonstrate the superiority of population state metrics over performance metrics in two situations, and in the process find evidence of XCS's strong bias against overlapping rules.},
  day = {01},


/home/david/Literatur/flexibleinstances.bib
@online{flexibleinstances,
  author = {{GHC Team}},
  title = {Flexible instances},
  titleaddon = {in {Glasgow Haskell Compiler} Users Guide},
  url = {https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/glasgow_exts.html#ghc-flag--XFlexibleInstances},
  urldate = {2017-07-13}
}
  year = {2014},


/home/david/Literatur/kovacs1996/kovacs1996.bib
@mastersthesis{kovacs1996,
  title = {Evolving Optimal Populations with {XCS} Classifier Systems},
  author = {Kovacs, Tim},
  year = 1996,
  school = {University of Birmingham},
  address = {Birmingham, UK},
}


/home/david/Literatur/typefamilies.bib
@online{typefamilies,
  author = {{GHC Team}},
  title = {Type families},
  titleaddon = {in {Glasgow Haskell Compiler} Users Guide},
  url = {https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/glasgow_exts.html\#type-families},
  urldate = {2017-07-14},
}
  year = {2015},


/home/david/Literatur/butz20013/butz20013.bib
@inproceedings{butz20013,
  address = {San Francisco, CA, USA},
  author = {Butz, Martin V. and Pelikan, Martin},
  booktitle = {Proceedings of the 3rd Annual Conference on Genetic and Evolutionary Computation},
  location = {San Francisco, California},
  pages = {935--942},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series = {GECCO '01},
  title = {Analyzing the Evolutionary Pressures in XCS},
  year = {2001},
}
  numpages = {8},
  acmid = {2955411},
  isbn = {1-55860-774-9},
  url = {http://dl.acm.org/citation.cfm?id=2955239.2955411},


/home/david/Literatur/karlsen2019/karlsen2019.bib
@article{karlsen2019,
  title={Learning versus optimal intervention in random {Boolean} networks},
  author={Karlsen, Matthew R. and Moschoyiannis, Sotiris K. and Georgiev, Vlad B.},
  journal={Applied Network Science},
  volume={4},
  number={1},
  pages={129--157},
  year={2019},
  publisher={Springer}
}


/home/david/Literatur/bull2005b/bull2005b.bib
@inbook{bull2005b,
  address = {Berlin, Heidelberg},
  author = {Bull, Larry and Kovacs, Tim},
  booktitle = {Foundations of Learning Classifier Systems},
  editor = {Bull, Larry and Kovacs, Tim},
  pages = {1--17},
  publisher = {Springer Berlin Heidelberg},
  title = {Foundations of Learning Classifier Systems: An Introduction},
  year = {2005},
}
abstract = {[Learning] Classifier systems are a kind of rule-based system with general mechanisms for processing rules in parallel, for adaptive generation of new rules, and for testing the effectiveness of existing rules. These mechanisms make possible performance and learning without the ``brittleness'' characteristic of most expert systems in AI.},
isbn = {978-3-540-32396-9},
doi = {10.1007/11319122_1},
url = {https://doi.org/10.1007/11319122_1},


/home/david/Literatur/aenugu2019/aenugu2019.bib
@inproceedings{aenugu2019,
    author = {Aenugu, Sneha and Spector, Lee},
    title = {Lexicase Selection in Learning Classifier Systems},
    year = {2019},
    isbn = {9781450361118},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3321707.3321828},
    doi = {10.1145/3321707.3321828},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
    pages = {356--364},
    numpages = {9},
    keywords = {learning classifier systems, lexicase selection, parent selection},
    location = {Prague, Czech Republic},
    series = {GECCO ’19},
}


/home/david/Literatur/scikit-learn.bib
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011},
}



/home/david/Literatur/drugowitsch2006b/drugowitsch2006b.bib
@techreport{drugowitsch2006b,
  author = {Jan Drugowitsch and Alwyn M. Barry},
  institution = {Department of Computer Science, University of Bath},
  issn = {1740-9497},
  month = {Jan},
  number = {CSBU-2006-02},
  title = {A Formal Framework for Reinforcement Learning with Function Approximation in Learning Classifier Systems},
  year = {2006},
}
  url = {https://drugowitschlab.hms.harvard.edu/files/drugowitschlab/files/tr2006b.pdf},


/home/david/Literatur/rudolph2019/rudolph2019.bib
@article{rudolph2019,
  author    = {Stefan Rudolph and Sven Tomforde and J{\"o}rg H{\"a}hner},
  title     = {Mutual influence-aware runtime learning of self-adaptation behavior},
  journal   = {ACM Transactions on Autonomous and Adaptive Systems},
  volume    = {14},
  number    = {1},
  pages     = {4},
  doi       = {10.1145/3345319},
  year      = {2019},
}


/home/david/Literatur/cdoerr2018/cdoerr2018.bib
@article{cdoerr2018,
  author    = {Carola Doerr and Markus Wagner},
  eprint    = {1803.01425},
  journal   = {CoRR},
  timestamp = {Mon, 13 Aug 2018 16:47:46 +0200},
  title     = {On the Effectiveness of Simple Success-Based Parameter Selection Mechanisms for Two Classical Discrete Black-Box Optimization Benchmark Problems},
  url       = {http://arxiv.org/abs/1803.01425},
  volume    = {abs/1803.01425},
  year      = {2018},
}
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-01425},


/home/david/Literatur/masoudnia2012/masoudnia2012.bib
@article{masoudnia2014,
  author = {Masoudnia, Saeed and Ebrahimpour, Reza},
  doi = {10.1007/s10462-012-9338-y},
  issn = {1573-7462},
  journal = {Artificial Intelligence Review},
  month = {Aug},
  number = {2},
  pages = {275--293},
  title = {Mixture of experts: a literature survey},
  url = {https://doi.org/10.1007/s10462-012-9338-y},
  volume = {42},
  year = {2014},
}
abstract = {Mixture of experts (ME) is one of the most popular and interesting combining methods, which has great potential to improve performance in machine learning. ME is established based on the divide-and-conquer principle in which the problem space is divided between a few neural network experts, supervised by a gating network. In earlier works on ME, different strategies were developed to divide the problem space between the experts. To survey and analyse these methods more clearly, we present a categorisation of the ME literature based on this difference. Various ME implementations were classified into two groups, according to the partitioning strategies used and both how and when the gating network is involved in the partitioning and combining procedures. In the first group, The conventional ME and the extensions of this method stochastically partition the problem space into a number of subspaces using a special employed error function, and experts become specialised in each subspace. In the second group, the problem space is explicitly partitioned by the clustering method before the experts' training process starts, and each expert is then assigned to one of these sub-spaces. Based on the implicit problem space partitioning using a tacit competitive process between the experts, we call the first group the mixture of implicitly localised experts (MILE), and the second group is called mixture of explicitly localised experts (MELE), as it uses pre-specified clusters. The properties of both groups are investigated in comparison with each other. Investigation of MILE versus MELE, discussing the advantages and disadvantages of each group, showed that the two approaches have complementary features. Moreover, the features of the ME method are compared with other popular combining methods, including boosting and negative correlation learning methods. As the investigated methods have complementary strengths and limitations, previous researches that attempted to combine their features in integrated approaches are reviewed and, moreover, some suggestions are proposed for future research directions.},
  day = {01},


/home/david/Literatur/nakata2019/nakata2019.bib
@inproceedings{nakata2019,
    author = {Nakata, Masaya and Browne, Will N.},
    title = {How {XCS} Can Prevent Misdistinguishing Rule Accuracy: A Preliminary Study},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319619.3321942},
    doi = {10.1145/3319619.3321942},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {183--184},
    numpages = {2},
    keywords = {theory, learning classifier system, rule-generation},
    location = {Prague, Czech Republic},
    series = {GECCO ’19}
}


/home/david/Literatur/risi2020/risi2020.bib
@article{risi2020,
  author    = {Sebastian Risi and Mike Preuss},
  title     = {From Chess and Atari to {StarCraft} and Beyond: How Game {AI} is Driving the World of {AI}},
  journal   = {{KI}},
  volume    = {34},
  number    = {1},
  pages     = {7--17},
  year      = {2020},
  url       = {https://doi.org/10.1007/s13218-020-00647-w},
  doi       = {10.1007/s13218-020-00647-w},
  timestamp = {Mon, 09 Mar 2020 15:50:59 +0100},
  biburl    = {https://dblp.org/rec/journals/ki/RisiP20a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



/home/david/Literatur/tharakunnel2003/tharakunnel2003.bib
@inproceedings{tharakunnel2003,
  address = {Berlin, Heidelberg},
  author = {Kurian K. Tharakunnel and Martin V. Butz and David E. Goldberg},
  booktitle = {Genetic and Evolutionary Computation --- GECCO 2003},
  editor = {Cant{\'u}-Paz, Erick and Foster, James A. and Deb, Kalyanmoy and Davis, Lawrence David and Roy, Rajkumar and O'Reilly, Una-May and Beyer, Hans-Georg and Standish, Russell and Kendall, Graham and Wilson, Stewart and Harman, Mark and Wegener, Joachim and Dasgupta, Dipankar and Potter, Mitch A. and Schultz, Alan C. and Dowsland, Kathryn A. and Jonoska, Natasha and Miller, Julian},
  isbn = {978-3-540-45110-5},
  pages = {1906--1917},
  publisher = {Springer Berlin Heidelberg},
  title = {Towards Building Block Propagation in XCS: A Negative Result and Its Implications},
  year = {2003},
}
  abstract = {The accuracy-based classifier system XCS is currently the most successful learning classifier system. Several recent studies showed that XCS can produce machine-learning competitive results. Nonetheless, until now the evolutionary mechanisms in XCS remained somewhat ill-understood. This study investigates the selectorecombinative capabilities of the current XCS system. We reveal the accuracy dependence of XCS's evolutionary algorithm and identify a fundamental limitation of the accuracy-based fitness approach in certain problems. Implications and future research directions conclude the paper.},



/home/david/Literatur/franco2010/franco2010.bib
@inproceedings{franco2010,
  acmid = {1830672},
  address = {New York, NY, USA},
  author = {Franco, Mar\'{\i}a A. and Krasnogor, Natalio and Bacardit, Jaume},
  booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
  doi = {10.1145/1830483.1830672},
  isbn = {978-1-4503-0072-8},
  location = {Portland, Oregon, USA},
  numpages = {8},
  pages = {1039--1046},
  publisher = {ACM},
  series = {GECCO '10},
  title = {Speeding Up the Evaluation of Evolutionary Learning Systems Using {GPGPUs}},
  url = {http://doi.acm.org/10.1145/1830483.1830672},
  year = {2010},
}


/home/david/Literatur/holland1976/holland1976.bib
@article{holland1976,
author = {John H. Holland},
year = {1976},
title = {Adaptation},
editor = {Robert Rosen and Fred M. Snell},
journal = {Progress in theoretical biology},
volume = {4},
pages = {263--293},
}
New York: Plenum. (acc. to wilson1995)
New York: Pergamon Press (acc. to worldcat.org)


/home/david/Literatur/smartconstructors.bib
@online{smartconstructors,
  author = {{Haskell Wiki}},
  label = {Wik},
  title = {Smart constructors},
  titleaddon = {in the {Haskell} Wiki},
  url = {https://wiki.haskell.org/Smart_constructors},
  urldate = {2017-07-13},
  year = {2015},
}


/home/david/Literatur/debie2017/debie2017.bib
@article{debie2017,
  author = {Debie, Essam and Shafi, Kamran},
  journal = {Pattern Analysis and Applications},
  month = {Aug},
  title = {Implications of the curse of dimensionality for supervised learning classifier systems: theoretical and empirical analyses},
  year = {2017},
  pages = {1--18},
}
  day = {21},
  abstract = {Learning classifier systems are leading evolutionary machine learning systems that employ genetic algorithms to search for a set of optimally general and correct classification rules for a variety of machine learning problems, including supervised classification data mining problems. The curse of dimensionality is a phenomenon that arises when analysing data in high-dimensional spaces. Performance issues when dealing with increasing dimensionality in the training data, such as poor classification accuracy and stalled genetic search, are well known for learning classifier systems. However, a systematic study to establish the relationship between increasing dimensionality and learning challenges in these systems is lacking. The aim of this paper is to analyse the behaviour of Michigan-style learning classifier systems that use the most commonly adopted and expressive interval-based rules representation, under curse of dimensionality (also known as Hughes Phenomenon) problem. In this paper, we use well-established and mathematically founded formal geometrical properties of high-dimensional data spaces and generalisation theory of these systems to propose a formulation of such relationship. The proposed formulations are validated experimentally using a set of synthetic, two-class classification problems. The findings demonstrate that the curse of dimensionality occurs for as few as ten dimensions and negatively affects the evolutionary search with a hyper-rectangular rule representation. A number of approaches to overcome some of the difficulties uncovered by the presented analysis are then discussed. Three approaches are then analysed in more detail using a set of synthetic, two-class classification problems. Experimental study demonstrates the effectiveness of these approaches to handle increasing dimensional data.},
  issn = {1433-755X},
  doi = {10.1007/s10044-017-0649-0},
  url = {https://doi.org/10.1007/s10044-017-0649-0},



/home/david/Literatur/cirincione2019/cirincione2019.bib
@inproceedings{cirincione2019,
    author = {Gregory Cirincione and Dinesh Verma},
    booktitle = {Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications},
    doi = {10.1117/12.2526661},
    editor = {Tien Pham},
    keywords = {federated machine learning, artificial intelligence, multi-domain operations, tactical edge, coalitions, distributed machine learning, operational complexity, operational frameworlk},
    organization = {International Society for Optics and Photonics},
    pages = {29 -- 48},
    publisher = {SPIE},
    title = {{Federated machine learning for multi-domain operations at the tactical edge}},
    url = {https://doi.org/10.1117/12.2526661},
    volume = {11006},
    year = {2019},
}


/home/david/Literatur/matsumoto2018/matsumoto2018.bib
@inproceedings{matsumoto2018,
    author = {Matsumoto, Kazuma and Takano, Ryo and Tatsumi, Takato and Sato, Hiroyuki and Kovacs, Tim and Takadama, Keiki},
    title = {{XCSR} Based on Compressed Input by Deep Neural Network for High Dimensional Data},
    year = {2018},
    isbn = {9781450357647},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3205651.3208281},
    doi = {10.1145/3205651.3208281},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {1418--1425},
    numpages = {8},
    keywords = {deep learning, neural network, XCS, XCSR, LCS},
    location = {Kyoto, Japan},
    series = {GECCO ’18},
}

  




/home/david/Literatur/lanzi2001/lanzi2001.bib
@inproceedings{lanzi2001,
  acmid = {2955414},
  address = {San Francisco, CA, USA},
  author = {Lanzi, Pier Luca},
  booktitle = {Proceedings of the 3rd Annual Conference on Genetic and Evolutionary Computation},
  isbn = {1-55860-774-9},
  location = {San Francisco, California},
  numpages = {8},
  pages = {958--965},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series = {GECCO'01},
  title = {Mining Interesting Knowledge from Data with the XCS Classifier System},
  url = {http://dl.acm.org/citation.cfm?id=2955239.2955414},
  year = {2001},
}


/home/david/Literatur/pätzel2019/pätzel2019.bib
@inproceedings{paetzel2019,
    author = {P\"{a}tzel, David and Stein, Anthony and H\"{a}hner, J\"{o}rg},
    title = {A Survey of Formal Theoretical Advances Regarding {XCS}},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319619.3326848},
    doi = {10.1145/3319619.3326848},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {1295--1302},
    numpages = {8},
    keywords = {xcs, hyperparameter, evolutionary machine learning, learning classifier systems, formalisation, formal theory},
    location = {Prague, Czech Republic},
    series = {GECCO ’19}
}

  



/home/david/Literatur/haskell2010.bib
@online{haskell2010,
  author  = {Marlow, Simon and others},
  editor  = {Marlow, Simon},
  title   = {{Haskell} 2010 – Language Report},
  year    = {2010},
  url     = {https://www.haskell.org/onlinereport/haskell2010/},
  urldate = {2018-03-16},
  lastaccessed = {2018-03-16},
}
  (accessed 2018-03-16)
  month   = {April},


/home/david/Literatur/kumar2019/kumar2019.bib
@inproceedings{kumar2019,
    author = {Amod Kumar and Ashwni Bansal},
    booktitle = {2019 4th International Conference on Internet of Things: Smart Innovation and Usages ({IoT-SIU})},
    title = {Software Fault Proneness Prediction Using Genetic Based Machine Learning Techniques},
    year = {2019},
    pages = {1--5},
    doi = {https://doi.org/10.1109/IoT-SIU.2019.8777494},
    month = {April},
}
    volume = {},
    number = {},
    ISSN = {null},
    keywords = {genetic algorithms;learning (artificial intelligence);object-oriented methods;pattern classification;public domain software;software development management;software fault tolerance;software metrics;open source software;object oriented metrics;machine learning;genetic based method;SDLC;software replica;software fault proneness prediction;object oriented data;stepwise algorithm;data collection technique;learning classifier systems;Software;Measurement;Machine learning;Genetics;Object oriented modeling;Zero current switching;Predictive models;Software defect Prediction;Fault Prediction;Software Fault Proneness;Learning Classifier Systems;Genetic Based Machine Learning},


/home/david/Literatur/nakata2015/nakata2015.bib
@inproceedings{nakata2015,
  ISSN = {1089-778X},
  author = {Masaya Nakata and Pier Luca Lanzi and Tim Kovacs and Will Neil Browne and Keiki Takadama},
  booktitle = {2015 IEEE Congress on Evolutionary Computation (CEC)},
  doi = {10.1109/CEC.2015.7257264},
  month = {May},
  number = {},
  pages = {3012-3019},
  title = {How should learning classifier systems cover a state-action space?},
  volume = {},
  year = {2015},
}


/home/david/Literatur/liang1995/liang1995.bib
@inproceedings{liang1995,
  author = {Liang, Sheng and Hudak, Paul and Jones, Mark P.},
  title = {Monad Transformers and Modular Interpreters},
  booktitle = {Proceedings of the 22nd {ACM} {SIGPLAN-SIGACT} Symposium on Principles of Programming Languages},
  booksubtitle = {San Francisco, California, USA, January 23-25, 1995},
  year = {1995},
  pages = {333--343},
  publisher = {{ACM}},
  editor    = {Ron K. Cytron and Peter Lee},
}
  series = {POPL '95},
  address = {New York, NY, USA},
 isbn = {0-89791-692-1},
 numpages = {11},
 acmid = {199528},
 conference location = {San Francisco, California, USA},
  booktitle = {Conference Record of POPL'95: 22nd {ACM} {SIGPLAN-SIGACT} Symposium on Principles of Programming Languages, San Francisco, California, USA, January 23-25, 1995},
  crossref  = {DBLP:conf/popl/1995},
  url       = {http://doi.acm.org/10.1145/199448.199528},
  doi       = {10.1145/199448.199528},
  biburl    = {https://dblp.org/rec/bib/conf/popl/LiangHJ95},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  proceedings{DBLP:conf/popl/1995,
  title     = {Conference Record of POPL'95: 22nd {ACM} {SIGPLAN-SIGACT} Symposium on Principles of Programming Languages, San Francisco, California, USA, January 23-25, 1995},
  publisher = {{ACM} Press},
  year      = {1995},
  url       = {http://dl.acm.org/citation.cfm?id=199448},
  isbn      = {0-89791-692-1},
  timestamp = {Mon, 10 Dec 2012 15:26:39 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/popl/1995},
  bibsource = {dblp computer science bibliography, https://dblp.org}


/home/david/Literatur/kozlowski2019/kozlowski2019.bib
@inproceedings{kozlowski2019,
    author = {Kozlowski, Norbert and Unold, Olgierd},
    title = {Preliminary Tests of a Real-Valued Anticipatory Classifier System},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319619.3326797},
    doi = {10.1145/3319619.3326797},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {1289--1294},
    numpages = {6},
    keywords = {OpenAI Gym, anticipatory learning classifier systems},
    location = {Prague, Czech Republic},
    series = {GECCO ’19},
}


/home/david/Literatur/wadler1989/wadler1989.bib
@inproceedings{wadler1989,
 acmid = {99404},
 address = {New York, NY, USA},
 author = {Wadler, Philip},
 booktitle = {Proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture},
 doi = {10.1145/99370.99404},
 isbn = {0-89791-328-0},
 location = {Imperial College, London, United Kingdom},
 numpages = {13},
 pages = {347--359},
 publisher = {ACM},
 series = {FPCA '89},
 title = {Theorems for Free!},
 url = {http://doi.acm.org/10.1145/99370.99404},
 year = {1989},
}


/home/david/Literatur/butz2009/butz2009.bib
@Article{butz2009,
  author = {Butz, Martin V. and Lanzi, Pier Luca},
  journal = {Evolutionary Intelligence},
  month = {May},
  number = {3},
  pages = {141--147},
  title = {Sequential problems that test generalization in learning classifier systems},
  volume = {2},
  year = {2009},
}
abstract = {We present an approach to build sequential decision making problems which can test the generalization capabilities of classifier systems. The approach can be applied to any sequential problem defined over a binary domain and it generates a new problem with bounded sequential difficulty and bounded generalization difficulty. As an example, we applied the approach to generate two problems with simple sequential structure, huge number of states (more than a million), and many generalizations. These problems are used to compare a classifier system with effective generalization (XCS) and a learner without generalization (Q-learning). The experimental results confirm what was previously found mainly using single-step problems: also in sequential problems with huge state spaces, XCS can generalize effectively by detecting those substructures that are necessary for optimal sequential behavior.},
  url = {https://doi.org/10.1007/s12065-009-0019-y},
  doi = {10.1007/s12065-009-0019-y},
  issn = {1864-5917},
  day = {19},


/home/david/Literatur/sukhbaatar2016/sukhbaatar2016.bib
@incollection{sukhbaatar,
    title = {Learning Multiagent Communication with Backpropagation},
    author = {Sukhbaatar, Sainbayar and szlam, arthur and Fergus, Rob},
    booktitle = {Advances in Neural Information Processing Systems 29},
    editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
    pages = {2244--2252},
    year = {2016},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf},
}


/home/david/Literatur/kovacs2004/kovacs2004.bib
@inbook{kovacs2004,
  address = {London},
  author = {Kovacs, Tim},
  bookTitle = {Strength or Accuracy: Credit Assignment in Learning Classifier Systems},
  doi = {10.1007/978-0-85729-416-6_4},
  isbn = {978-0-85729-416-6},
  pages = {97--123},
  publisher = {Springer London},
  title = {What Should a Classifier System Learn?},
  url = {https://doi.org/10.1007/978-0-85729-416-6_4},
  year = {2004},
}
  abstract = {In this Chapter we consider the issues of how a classifier system should learn to represent a Boolean function, and how we should measure its progress in doing so. We identify four properties which may be desirable of a representation; that it be complete, accurate, minimal and non-overlapping, and distinguish variations on two of these properties for the XCS system. We distinguish two categories of learning metric, introduce new metrics and evaluate them. We demonstrate the superiority of population state metrics over perfor- mance metrics in two situations, and in the process find evidence of XCS's strong bias against overlapping rules.},


/home/david/Literatur/nakata2018/nakata2018.bib
@inproceedings{nakata2018,
  author = {Nakata, Masaya and Browne, Will N. and Hamagami, Tomoki},
  title = {Theoretical Adaptation of Multiple Rule-generation in XCS},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  series = {GECCO '18},
  year = {2018},
  location = {Kyoto, Japan},
  pages = {482--489},
  publisher = {ACM},
  address = {New York, NY, USA},
}
  numpages = {8},
  keywords = {learning classifier system, rule-generation, theory},
  isbn = {978-1-4503-5618-3},
  url = {http://doi.acm.org/10.1145/3205455.3205465},
  doi = {10.1145/3205455.3205465},
  acmid = {3205465},


/home/david/Literatur/urbanowicz2009/urbanowicz2009.bib
@article{urbanowicz2009,
  author = {Ryan J. Urbanowicz and Jason H. Moore},
  journal = {Journal of Artificial Evolution and Applications},
  title = {Learning Classifier Systems: A Complete Introduction, Review, and Roadmap},
  volume = {2009},
  year = {2009},
}
Article ID 736398
25 pages


/home/david/Literatur/butz20012/butz20012.bib
@techreport{butz20012,
  author = {Martin V. Butz and Martin Pelikan},
  institution = {Department of Computer Science, University of Illinois at Urbana-Champaign},
  number = {2001009},
  title = {Analyzing the Evolutionary Pressures in XCS},
  year = {2004},
}


/home/david/Literatur/drugowitsch2006a/drugowitsch2006a.bib
@techreport{drugowitsch2006a,
  author = {Jan Drugowitsch and Alwyn M. Barry},
  institution = {Department of Computer Science, University of Bath},
  issn = {1740-9497},
  month = {Jan},
  number = {CSBU-2006-01},
  title = {A Formal Framework and Extensions for Function Approximation in Learning Classifier Systems},
  year = {2006},
}
  url = {https://drugowitschlab.hms.harvard.edu/files/drugowitschlab/files/tr2006b.pdf},


/home/david/Literatur/stein2017/stein2017.bib
@incollection{stein2017,
  author    = {Anthony Stein and Roland Maier and J{\"o}rg H{\"a}hner},
  title     = {Toward curious learning classifier systems: combining XCS with active learning concepts},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion - GECCO '17, Berlin, Germany — July 15 - 19, 2017},
  isbn      = {9781450349390},
  doi       = {10.1145/3067695.3082488},
  year      = {2017},
}


/home/david/Literatur/butz2003c/butz2003c.bib
@inproceedings{butz2003c,
  author = {Butz, Martin V. and Goldberg, David E.},
  editor = {Cant{\'u}-Paz, Erick and Foster, James A. and Deb, Kalyanmoy and Davis, Lawrence David and Roy, Rajkumar and O'Reilly, Una-May and Beyer, Hans-Georg and Standish, Russell and Kendall, Graham and Wilson, Stewart and Harman, Mark and Wegener, Joachim and Dasgupta, Dipankar and Potter, Mitch A. and Schultz, Alan C. and Dowsland, Kathryn A. and Jonoska, Natasha and Miller, Julian},
  title = {Bounding the Population Size in XCS to Ensure Reproductive Opportunities},
  booktitle = {Genetic and Evolutionary Computation --- GECCO 2003},
  year = {2003},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {1844--1856},
  isbn = {978-3-540-45110-5},
}
  abstract = {Despite several recent successful comparisons and applications of the accuracy-based learning classifier system XCS, it is hardly understood how crucial parameters should be set in XCS nor how XCS can be expect to scale up in larger problems. Previous research identified a covering challenge in XCS that needs to be obeyed to ensure that the genetic learning process takes place. Furthermore, a schema challenge was identified that, once obeyed, ensures the existence of accurate classifiers. This paper departs from these challenges deriving a reproductive. opportunity bound. The bound assures that more accurate classifiers get a chance for reproduction. The relation to the previous bounds as well as to the specificity pressure in XCS are discussed as well. The derived bound shows that XCS scales in a machine learning competitive way.},


/home/david/Literatur/stein2016/stein2016.bib
@article{stein2016,
  author    = {Anthony Stein and Dominik Rauh and Sven Tomforde and J{\"o}rg H{\"a}hner},
  title     = {Augmenting the Algorithmic Structure of XCS by Means of Interpolation},
  journal   = {Lecture Notes in Computer Science},
  volume    = {9637},
  pages     = {348 -- 360},
  doi       = {10.1007/978-3-319-30695-7\_26},
  year      = {2016},
}


/home/david/Literatur/andreas2017/andreas2017.bib
@InProceedings{andreas2017,
  title = 	 {Modular Multitask Reinforcement Learning with Policy Sketches},
  author =       {Jacob Andreas and Dan Klein and Sergey Levine},
  pages = 	 {166--175},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/andreas17a/andreas17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/andreas17a.html},
}
  abstract = 	 {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.},


/home/david/Literatur/nguyen2019b/nguyen2019b.bib
@inproceedings{nguyen2019b,
    author={Trung B. {Nguyen} and Will N. {Browne} and Mengjie {Zhang}},
    booktitle={2019 {IEEE} Congress on Evolutionary Computation ({CEC})},
    title={Online Feature-Generation of Code Fragments for {XCS} to Guide Feature Construction},
    year={2019},
    volume={},
    number={},
    pages={3308--3315},
    keywords={feature extraction;feature selection;genetic algorithms;learning (artificial intelligence);pattern classification;trees (mathematics);XCS;high-level features;feature construction;code fragments;learning classifier systems;online feature-generation;genetic programming-like trees;XCSCFC system;nontransfer learning;feature extraction;feature selection;Sociology;Statistics;Learning systems;Feature extraction;Task analysis;Search problems;Multiplexing;XCSCFC;XCS;code fragment},
    doi={10.1109/CEC.2019.8789950},
    month={June},
}
issn={null},


/home/david/Literatur/lanzi2002/lanzi2002.bib
@article{lanzi2002,
  author = {Lanzi, Pier Luca},
  issn = {1432-7643},
  journal = {Soft Computing},
  month = {Jun},
  number = {3},
  pages = {162--170},
  title = {Learning classifier systems from a reinforcement learning perspective},
  volume = {6},
  year = {2002},
}
  url = {https://doi.org/10.1007/s005000100113},
  doi = {10.1007/s005000100113},
  abstract = {{\enspace}We analyze learning classifier systems in the light of tabular reinforcement learning. We note that although genetic algorithms are the most distinctive feature of learning classifier systems, it is not clear whether genetic algorithms are important to learning classifiers systems. In fact, there are models which are strongly based on evolutionary computation (e.g., Wilson's XCS) and others which do not exploit evolutionary computation at all (e.g., Stolzmann's ACS). To find some clarifications, we try to develop learning classifier systems ``from scratch'', i.e., starting from one of the most known reinforcement learning technique, Q-learning. We first consider thebasics of reinforcement learning: a problem modeled as a Markov decision process and tabular Q-learning. We introduce a formal framework to define a general purpose rule-based representation which we use to implement tabular Q-learning. We formally define generalization within rules and discuss the possible approaches to extend our rule-based Q-learning with generalization capabilities. We suggest that genetic algorithms are probably the most general approach for adding generalization although they might be not the only solution.},
  day = {01},


/home/david/Literatur/haskell-implementations.bib
@online{haskell-implementations,
  author  = {Haskell community},
  label   = {Has},
  title   = {{Haskell} Implementations},
  month   = {February},
  year    = {2017},
  url     = {https://wiki.haskell.org/Implementations},
  urldate = {2018-03-16},
  lastaccessed = {2018-03-16},
}


/home/david/Literatur/orriols-puig2007b/orriols-puig2007b.bib
@inproceedings{orriols-puig2007b,
  address = {New York, NY, USA},
  author = {Orriols-Puig, Albert and Sastry, Kumara and Lanzi, Pier Luca and Goldberg, David E. and Bernad\'{o}-Mansilla, Ester},
  booktitle = {Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation},
  location = {London, England},
  pages = {1846--1853},
  publisher = {ACM},
  series = {GECCO '07},
  title = {Modeling Selection Pressure in XCS for Proportionate and Tournament Selection},
  year = {2007},
}
  keywords = {LCS, XCS, proportionate selection, tournament selection},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/1276958.1277325},
  isbn = {978-1-59593-697-4},
  doi = {10.1145/1276958.1277325},
  acmid = {1277325},


/home/david/Literatur/kovacs2013/kovacs2013.bib
@inproceedings{kovacs2013,
  address = {New York, NY, USA},
  author = {Kovacs, Tim and Tindale, Robin},
  booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
  location = {Amsterdam, The Netherlands},
  pages = {1069--1076},
  publisher = {ACM},
  series = {GECCO '13},
  title = {Analysis of the Niche Genetic Algorithm in Learning Classifier Systems},
  year = {2013},
}
  numpages = {8},
  acmid = {2465803},
  doi = {10.1145/2463372.2465803},
  isbn = {978-1-4503-1963-8},
  keywords = {learning classifier systems, niche genetic algorithm},
  url = {http://doi.acm.org/10.1145/2463372.2465803},


/home/david/Literatur/intervals.bib
@online{intervals,
  author = {Kmett, Edward and others},
  title = {The \emph{intervals} {Haskell} package},
  year = {2017},
  url = {https://hackage.haskell.org/package/intervals},
  urldate = {2017-07-14}
}


/home/david/Literatur/holland1975/holland1975.bib
@book{holland1975,
  address = {Ann Arbor, MI, USA},
  author = {Holland, John H.},
  note = {second edition, 1992},
  publisher = {University of Michigan Press},
  title = {Adaptation in Natural and Artificial Systems},
  year = {1975},
}


/home/david/Literatur/stone2005/stone2005.bib
@inbook{stone2005,
  address = {Berlin, Heidelberg},
  author = {Stone, Christopher and Bull, Larry},
  booktitle = {Foundations of Learning Classifier Systems},
  editor = {Bull, Larry and Kovacs, Tim},
  pages = {127--175},
  publisher = {Springer Berlin Heidelberg},
  title = {An Analysis of Continuous-Valued Representations for Learning Classifier Systems},
  year = {2005},
}
  abstract = {Learning Classifier Systems [11] typically use a ternary representation to encode the environmental condition that a classifier matches. However, many real-world problems are not conveniently expressed in terms of a ternary representation and several alternate representations have been suggested to allow Learning Classifier Systems to handle these problems more readily [1, 3, 6, 15].},
  doi = {10.1007/11319122_6},
  url = {https://doi.org/10.1007/11319122_6},
  isbn = {978-3-540-32396-9},



/home/david/Literatur/stalph2012/stalph2012.bib
@article{stalph2012,
  author    = {Patrick O. Stalph and Xavier Llor{\`{a}} and David E. Goldberg and Martin V. Butz},
  editor    = {Per Kristian Lehre and Frank Neumann and Jonathan E. Rowe and Xin Yao},
  journal   = {Theoretical Computer Science},
  pages     = {126--141},
  title     = {Resource management and scalability of the {XCSF} learning classifier system},
  volume    = {425},
  year      = {2012},
}
  url       = {https://doi.org/10.1016/j.tcs.2010.07.007},
  doi       = {10.1016/j.tcs.2010.07.007},
  biburl    = {https://dblp.org/rec/bib/journals/tcs/StalphLGB12},
  bibsource = {dblp computer science bibliography, https://dblp.org},


/home/david/Literatur/drugowitsch2008a/drugowitsch2008a.bib
@article{drugowitsch2008a,
  author    = {Jan Drugowitsch and Alwyn M. Barry},
  title     = {A formal framework and extensions for function approximation in learning classifier systems},
  journal   = {Machine Learning},
  volume    = {70},
  number    = {1},
  pages     = {45--88},
  year      = {2008},
}
  url       = {https://doi.org/10.1007/s10994-007-5024-8},
  doi       = {10.1007/s10994-007-5024-8},
  timestamp = {Sun, 28 May 2017 13:18:23 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/ml/DrugowitschB08},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  month="Jan",
  day="01",
  issn="1573-0565",


/home/david/Literatur/urbanowicz2017/urbanowicz2017.bib
@book{urbanowicz2017,
  author    = {Ryan J. Urbanowicz and Will N. Browne},
  title     = {Introduction to Learning Classifier Systems},
  series    = {Springer Briefs in Intelligent Systems},
  publisher = {Springer},
  year      = {2017},
  url       = {https://doi.org/10.1007/978-3-662-55007-6},
  doi       = {10.1007/978-3-662-55007-6},
  isbn      = {978-3-662-55006-9},
  timestamp = {Mon, 21 Aug 2017 13:26:52 +0200},
  biburl    = {https://dblp.org/rec/series/sbis/UrbanowiczB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


/home/david/Literatur/butz2002/butz2002.bib
@article{butz2002,
  author = {Butz, Martin V. and Wilson, Stewart W.},
  journal = {Soft Computing},
  month = {Jun},
  number = {3},
  pages = {144--153},
  title = {An algorithmic description of {XCS}},
  volume = {6},
  year = {2002},
}
  abstract = {{\enspace}A concise description of the XCS classifier system's parameters, structures, and algorithms is presented as an aid to research. The algorithms are written in modularly structured pseudo code with accompanying explanations.},
  url = {https://doi.org/10.1007/s005000100111},
  doi = {10.1007/s005000100111},
  issn = {1432-7643},
  day = {01},



/home/david/Literatur/tatsumi2019b/tatsumi2019b.bib
@INPROCEEDINGS{tatsumi2019b,
    author={Takato {Tatsumi} and Keiki {Takadama}},
    booktitle={2019 {IEEE} Congress on Evolutionary Computation ({CEC})},
    title={Comparison of Statistical Table- and Non-Statistical Table-based XCS in Noisy Environments},
    year={2019},
    volume={},
    number={},
    pages={1875--1882},
    keywords={learning (artificial intelligence);pattern classification;statistical analysis;statistical table based XCS;nonstatistical table-based XCS;accuracy based learning classifier system;Uncertainty;Standards;Impedance matching;Genetic algorithms;Convergence;Estimation;Noise measurement;learning classifier system;accuracy;input noise;output noise;reward noise},
    doi={10.1109/CEC.2019.8790325},
    month={June},
}
ISSN={null},


